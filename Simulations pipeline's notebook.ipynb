{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a1a9b6-b8e4-427b-86a3-07142e762cea",
   "metadata": {},
   "source": [
    "# Installs  and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f4fab-8caa-475b-ae7f-41cbf369deb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install shap catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c842d9-d925-4a0d-a55c-47c61f70cf81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "from scipy.spatial.distance import euclidean\n",
    "import pickle\n",
    "import os\n",
    "import shap\n",
    "import catboost\n",
    "import s3fs\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceda784-dd4f-40a8-8655-9d55fa9606be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4598cca1-8c20-4eec-b0a8-19de99d9c9a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49495ee-c311-413a-8805-ec76f735832e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d75d1e0-9dc2-422a-9d1e-3acf4a762742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_path_to_read_and_date(\n",
    "    read_last_date: bool,\n",
    "    bucket: str,\n",
    "    key: str,\n",
    "    partition_date: str,\n",
    "    n_partition: int = 1,\n",
    "):\n",
    "    \"\"\"Get path to read (given or last) and the chosen date.\n",
    "\n",
    "    :param read_last_date: Boolean to read last valid date (True) or given date (False).\n",
    "    :param bucket: S3 bucket.\n",
    "    :param key: S3 key.\n",
    "    :param partition_date: String with the execution date (could be separated by '=' sign).\n",
    "    :param n_partition: 1 means select the last available partition, 2 the following one, and so on.\n",
    "    :return: Tuple[\n",
    "            Path with data,\n",
    "            year of the read data,\n",
    "            month of the read data,\n",
    "            day of the read data\n",
    "        ]\n",
    "    \"\"\"\n",
    "    if read_last_date:\n",
    "        exec_date = int(partition_date.split(\"=\")[-1].replace(\"-\", \"\"))\n",
    "        date_preffix = (\n",
    "            regex_split(r\"[0-9]{4}-?[0-9]{2}-?[0-9]{2}\", partition_date)[0]\n",
    "            if \"=\" in partition_date\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        path = get_last_s3_partition(\n",
    "            s3_dir=f\"{bucket}/{key}/\",\n",
    "            execution_date=exec_date,\n",
    "            preffix=date_preffix,\n",
    "            n_partition=n_partition,\n",
    "        )\n",
    "        date = path.split(\"/\")[-1].split(\"=\")[-1].replace(\"-\", \"\")\n",
    "        # date = partition_date.split(\"/\")[-1].split(\"=\")[-1].replace(\"-\", \"\")\n",
    "        year, month, day = date[:4], date[4:6], date[6:]\n",
    "        path = f\"s3://{path}\"\n",
    "    else:\n",
    "        path = f\"s3://{bucket}/{key}/{partition_date}\"\n",
    "        date = partition_date.split(\"=\")[-1]\n",
    "        if \"-\" in partition_date:\n",
    "            date = date.split(\"-\")\n",
    "            year, month, day = date[0], date[1], date[2]\n",
    "        else:\n",
    "            year, month, day = date[:4], date[4:6], date[6:]\n",
    "    return path, year, month, day\n",
    "\n",
    "def get_last_s3_partition(\n",
    "    s3_dir: str,\n",
    "    execution_date: int,\n",
    "    preffix: str,\n",
    "    n_partition: int = 1,\n",
    ") -> str:\n",
    "    \"\"\"This function get the las partitition of a given path from an specified execution_date.\n",
    "\n",
    "    :param s3_dir: S3 path data ending with '/'\n",
    "    :param execution_date: Execution date to limit the search perimeter.\n",
    "    :param preffix: Preffix of the s3 key for the date partition. (Could be 'insert_date_ci=').\n",
    "    :param n_partition: 1 means select the last available partition, 2 the following one, and so on.\n",
    "    :return: Complete path of the last partition to read.\n",
    "    \"\"\"\n",
    "    preffix = \" \" if preffix is None else preffix\n",
    "    print(preffix)\n",
    "    \n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_bucket = s3_dir.split(\"/\", 1)[0]\n",
    "    s3_prefix = s3_dir.split(\"/\", 1)[-1]\n",
    "    print(f's3_bucket: {s3_bucket}')\n",
    "    print(f's3_prefix: {s3_prefix}')\n",
    "    s3_contents = s3_client.list_objects_v2(\n",
    "        Bucket=s3_bucket, Prefix=s3_prefix, Delimiter=\"/\"\n",
    "    ).get(\"CommonPrefixes\")\n",
    "    print(f's3_contents: {s3_contents}')\n",
    "    partition_date_aux = [\n",
    "        int(\n",
    "            content[\"Prefix\"]\n",
    "            .strip(\"/\")\n",
    "            .split(\"/\")[-1]\n",
    "            .replace(\"-\", \"\")\n",
    "            .split(preffix)[-1]\n",
    "        )\n",
    "        for content in s3_contents\n",
    "    ]\n",
    "    partition_date = [\n",
    "        content[\"Prefix\"].strip(\"/\").split(\"/\")[-1].split(preffix)[-1]\n",
    "        for content in s3_contents\n",
    "    ]\n",
    "    filtered_dates = list(\n",
    "        filter(\n",
    "            lambda e: e[0] <= execution_date, zip(partition_date_aux, partition_date)\n",
    "        )\n",
    "    )\n",
    "    sorted_dates = sorted(filtered_dates, key=lambda e: e[0])\n",
    "    try:\n",
    "        return_path = os.path.join(s3_dir, f\"{preffix}{str(sorted_dates[-n_partition][-1])}\".strip())\n",
    "    except IndexError:\n",
    "        return_path = os.path.join(s3_dir, f\"{preffix}_notfoundpreviousdate\".strip())\n",
    "    return return_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e6e1c8-dce5-430f-bb6b-f56b354b66ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d017f0-6c3e-4a44-bbe2-4daa1c11f2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_nps(promoters, detractors, total_responses):\n",
    "    \"\"\"Calcula el Net Promoter Score (NPS).\"\"\"\n",
    "    if total_responses == 0:\n",
    "        return np.nan\n",
    "    return ((promoters - detractors) / total_responses) * 100\n",
    "\n",
    "def calculate_weighted_nps(group_df):\n",
    "    \"\"\"Calcula el NPS ponderado para un grupo de datos.\"\"\"\n",
    "    promoters_weight = group_df.loc[group_df['nps_100'] > 8, 'monthly_weight'].sum()\n",
    "    detractors_weight = group_df.loc[group_df['nps_100'] <= 6, 'monthly_weight'].sum()\n",
    "    total_weight = group_df['monthly_weight'].sum()\n",
    "    \n",
    "    if total_weight == 0:\n",
    "        return np.nan\n",
    "    return (promoters_weight - detractors_weight) / total_weight * 100\n",
    "\n",
    "def calculate_satisfaction(df, variable):\n",
    "    \"\"\"Calcula la tasa de satisfacción para una variable dada, utilizando pesos mensuales si están disponibles.\"\"\"\n",
    "    # Comprobar si la columna 'monthly_weight' existe y no está completamente vacía para los datos relevantes\n",
    "    if 'monthly_weight' in df.columns and not df[df[variable].notnull()]['monthly_weight'].isnull().all():\n",
    "        # Suma de los pesos donde la variable es >= 8 y satisface la condición de estar satisfecho\n",
    "        satisfied_weight = df[df[variable] >= 8]['monthly_weight'].sum()\n",
    "        # Suma de todos los pesos donde la variable no es NaN\n",
    "        total_weight = df[df[variable].notnull()]['monthly_weight'].sum()\n",
    "        # Calcula el porcentaje de satisfacción usando los pesos\n",
    "        if total_weight == 0:\n",
    "            return np.nan\n",
    "        return (satisfied_weight / total_weight) * 100\n",
    "    else:\n",
    "        # Contar respuestas satisfechas\n",
    "        satisfied_count = df[df[variable] >= 8].shape[0]\n",
    "        # Contar total de respuestas válidas\n",
    "        total_count = df[variable].notnull().sum()\n",
    "        # Calcula el porcentaje de satisfacción usando conteo\n",
    "        if total_count == 0:\n",
    "            return np.nan\n",
    "        return (satisfied_count / total_count) * 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_otp(df, n):\n",
    "    \"\"\"Calcula el On-Time Performance (OTP) como el porcentaje de valores igual a 1.\"\"\"\n",
    "    on_time_count = (df[f'otp{n}_takeoff'] == 0).sum()\n",
    "    total_count = df[f'otp{n}_takeoff'].notnull().sum()\n",
    "    return (on_time_count / total_count) * 100 if total_count > 0 else 0\n",
    "\n",
    "\n",
    "def calculate_load_factor(df, pax_column, capacity_column):\n",
    "    \"\"\"Calcula el factor de carga para una cabina específica.\"\"\"\n",
    "    total_pax = df[pax_column].sum()\n",
    "    total_capacity = df[capacity_column].sum()\n",
    "    # Evitar la división por cero\n",
    "    if total_capacity > 0:\n",
    "        return (total_pax / total_capacity) * 100\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def calculate_mean(df, variable):\n",
    "    \"\"\"Calcula la media de una variable dada.\"\"\"\n",
    "    return df[variable].mean()\n",
    "    \n",
    "def calculate_metrics_summary(df, start_date, end_date, touchpoints):\n",
    "    df['date_flight_local'] = pd.to_datetime(df['date_flight_local'])\n",
    "    # Filtrar por rango de fechas\n",
    "    df_filtered = df[(df['date_flight_local'] >= pd.to_datetime(start_date)) & (df['date_flight_local'] <= pd.to_datetime(end_date))]\n",
    "    \n",
    "    # Mapeo de cabinas a columnas de pax y capacidad\n",
    "    cabin_mapping = {\n",
    "        'Economy': ('pax_economy', 'capacity_economy'),\n",
    "        'Business': ('pax_business', 'capacity_business'),\n",
    "        'Premium Economy': ('pax_premium_ec', 'capacity_premium_ec')\n",
    "    }\n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    for (cabin, haul), group_df in df_filtered.groupby(['cabin_in_surveyed_flight', 'haul']):\n",
    "        \n",
    "        print(f'CABIN/HAUL: {cabin}/{haul}')\n",
    "        result = {\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'cabin_in_surveyed_flight': cabin,\n",
    "            'haul': haul,\n",
    "            'ticket_price': group_df['ticket_price'].mean(),\n",
    "            'load_factor': group_df['load_factor'].mean()*100,\n",
    "            'out_prob_nps': group_df['out_prob_nps'].mean()*100,\n",
    "            'uncertainty_nps': group_df['uncertainty_nps'].mean()*100,\n",
    "        }\n",
    "        \n",
    "        # Calcula el NPS para el grupo\n",
    "#         promoters = (group_df['nps_100'] >= 9).sum()\n",
    "#         detractors = (group_df['nps_100'] <= 6).sum()\n",
    "#         total_responses = group_df['nps_100'].notnull().sum()\n",
    "#         result['NPS'] = calculate_nps(promoters, detractors, total_responses) if total_responses else None\n",
    "        \n",
    "#         # Calcula el NPS ponderado para el grupo\n",
    "#         result['NPS_weighted'] = calculate_weighted_nps(group_df)\n",
    "        \n",
    "        # Satisfacción para cada touchpoint\n",
    "        for tp in touchpoints:\n",
    "            result[f'{tp}_satisfaction'] = calculate_satisfaction(group_df, tp)\n",
    "            result[f'{tp}_satisfaction_nps'] = group_df[f'{tp}_nps'].mean()*100       \n",
    "        \n",
    "        results_list.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199bfc01-18b7-44d5-85e6-d4e453b2bd35",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predict and explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4330495-15d7-4877-9789-a7cc460b290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_logit(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def calculate_SHAP_and_probability_binary(model_promoter, model_detractor, df):\n",
    "    # Extraer ID y fechas, manteniendo el índice\n",
    "    varibales_to_pass_through = ['respondent_id', 'cabin_in_surveyed_flight', 'haul', 'date_flight_local','otp15_takeoff','promoter_binary', 'detractor_binary', 'monthly_weight','cluster']\n",
    "    id_df = df[varibales_to_pass_through]\n",
    "    \n",
    "    # Preparar el conjunto de datos para predicciones, excluyendo ID y fechas\n",
    "    test_set = df.drop(varibales_to_pass_through, axis=1, errors='ignore')\n",
    "    \n",
    "    # Predicciones y probabilidades para promotores\n",
    "    promoter_test_set = test_set.drop(['promoter_binary'], axis=1, errors='ignore')\n",
    "    predictions_promoter = pd.DataFrame(model_promoter.predict(promoter_test_set), index=promoter_test_set.index, columns=[\"prediction_prom\"])\n",
    "    proba_promoter = pd.DataFrame(model_promoter.predict_proba(promoter_test_set)[:, 1], index=promoter_test_set.index, columns=[\"out_prob_prom\"])\n",
    "    \n",
    "    # Predicciones y probabilidades para detractores\n",
    "    detractor_test_set = test_set.drop(['detractor_binary'], axis=1, errors='ignore')\n",
    "    predictions_detractor = pd.DataFrame(model_detractor.predict(detractor_test_set), index=detractor_test_set.index, columns=[\"prediction_det\"])\n",
    "    proba_detractor = pd.DataFrame(model_detractor.predict_proba(detractor_test_set)[:, 1], index=detractor_test_set.index, columns=[\"out_prob_det\"])\n",
    "    \n",
    "    # Combinar resultados de predicción, manteniendo el índice original\n",
    "    prediction = pd.concat([id_df, test_set, predictions_promoter, proba_promoter, predictions_detractor, proba_detractor], axis=1)\n",
    "    \n",
    "    # SHAP values y explicadores para el modelo promotor\n",
    "    shap_Explainer_promoter = shap.TreeExplainer(model_promoter)\n",
    "    shap_values_promoter = shap_Explainer_promoter.shap_values(promoter_test_set)\n",
    "    feature_names = [i for i in promoter_test_set.columns]\n",
    "    shap_values_prom = pd.DataFrame(shap_values_promoter, index=promoter_test_set.index, columns=[f\"{i}_prom\" for i in feature_names])\n",
    "    shap_values_prom[\"base_value_prom\"] = shap_Explainer_promoter.expected_value\n",
    "    shap_values_prom[\"out_value_prom\"] = shap_values_prom.sum(axis=1)\n",
    "    \n",
    "    # SHAP values y explicadores para el modelo detractor\n",
    "    shap_Explainer_detractor = shap.TreeExplainer(model_detractor)\n",
    "    shap_values_detractor = shap_Explainer_detractor.shap_values(detractor_test_set)\n",
    "    shap_values_det = pd.DataFrame(shap_values_detractor, index=detractor_test_set.index, columns=[f\"{i}_det\" for i in feature_names])\n",
    "    shap_values_det[\"base_value_det\"] = shap_Explainer_detractor.expected_value\n",
    "    shap_values_det[\"out_value_det\"] = shap_values_det.sum(axis=1)\n",
    "    \n",
    "    # Combinar SHAP values con predicciones, manteniendo el índice original\n",
    "    output_df = pd.concat([prediction, shap_values_prom, shap_values_det], axis=1)\n",
    "    \n",
    "    # Devolver el dataframe de salida\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def from_shap_to_probability_binary(df, features_dummy, label_binary):\n",
    "    output_df = df.copy()\n",
    "    \n",
    "    # Determinar el sufijo basado en el tipo de modelo (promoter o detractor)\n",
    "    class_suffix = '_prom' if label_binary == 'promoter_binary' else '_det'\n",
    "    \n",
    "    # Identificar columnas de SHAP para la clase de interés, asumiendo que ya tienen el sufijo correcto\n",
    "    shap_columns = [col for col in df.columns if col.endswith(class_suffix)]\n",
    "    base_value_col = f'base_value{class_suffix}'\n",
    "    \n",
    "    # Convertir el valor base a probabilidades y actualizar el nombre de la columna\n",
    "    output_df[f'base_prob{class_suffix}'] = inv_logit(output_df[base_value_col])\n",
    "    \n",
    "    # Convertir valores SHAP a probabilidades sin cambiar los nombres de las columnas\n",
    "    for col in shap_columns:\n",
    "        output_df[col] = inv_logit(output_df[col])\n",
    "    \n",
    "    # Asegurarse de incluir solo las columnas relevantes en el DataFrame final\n",
    "    relevant_columns = ['respondent_id', 'cabin_in_surveyed_flight', 'haul', 'date_flight_local','otp15_takeoff'] + shap_columns + ['promoter_binary', 'detractor_binary', 'monthly_weight','cluster'] + [f'base_prob{class_suffix}'] + features_dummy\n",
    "    output_df = output_df[relevant_columns]\n",
    "    print(output_df)\n",
    "    return output_df\n",
    "\n",
    "def adjust_shap_values_binary(shap_values, base_prob, out_prob):\n",
    "    \"\"\"Ajustar los valores SHAP para un modelo binario basado en la distancia.\"\"\"\n",
    "    # Calcular la distancia total deseada entre la probabilidad base y la de salida\n",
    "    total_distance = out_prob - base_prob\n",
    "    # Calcular la suma total de los valores SHAP\n",
    "    total_shap = np.sum(shap_values)\n",
    "    # Calcular el factor de ajuste si la suma total de SHAP no es cero\n",
    "    adjustment_factor = total_distance / total_shap if total_shap != 0 else 0\n",
    "    # Ajustar los valores SHAP\n",
    "    return shap_values * adjustment_factor\n",
    "\n",
    "def from_shap_to_probability_binary(df, features_dummy, label_binary):\n",
    "    output_df = df.copy()\n",
    "    \n",
    "    # Determinar el sufijo basado en el tipo de modelo (promoter o detractor)\n",
    "    class_suffix = '_prom' if label_binary == 'promoter_binary' else '_det'\n",
    "    \n",
    "    # Identificar columnas de SHAP para la clase de interés, asumiendo que ya tienen el sufijo correcto\n",
    "    shap_columns = [f'{feature}{class_suffix}' for feature in features_dummy if f'{feature}{class_suffix}' in df.columns]\n",
    "    base_value_col = f'base_value{class_suffix}'\n",
    "    out_prob_col = f'out_prob{class_suffix}'\n",
    "\n",
    "    # Calcular la probabilidad base usando softmax o inv_logit según sea apropiado\n",
    "    output_df[f'base_prob{class_suffix}'] = inv_logit(output_df[base_value_col])\n",
    "\n",
    "    for index, row in output_df.iterrows():\n",
    "        # Extraer los valores SHAP para ajustar\n",
    "        shap_values = row[shap_columns].values\n",
    "        # Calcular los valores SHAP ajustados\n",
    "        adjusted_shap_values = adjust_shap_values_binary(shap_values, row[f'base_prob{class_suffix}'], row[out_prob_col])\n",
    "        # Actualizar el DataFrame con los valores SHAP ajustados\n",
    "        output_df.loc[index, shap_columns] = adjusted_shap_values\n",
    "\n",
    "    # Incluir solo las columnas relevantes en el DataFrame final\n",
    "    relevant_columns = ['respondent_id', 'cabin_in_surveyed_flight', 'haul', 'date_flight_local','otp15_takeoff'] + shap_columns + ['promoter_binary', 'detractor_binary', 'monthly_weight','cluster'] + [f'base_prob{class_suffix}', out_prob_col] + features_dummy\n",
    "    print(output_df)\n",
    "    output_df = output_df[relevant_columns]\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "# Function to create virtual ensembles\n",
    "def create_virtual_ensemble(model, total_trees, K):\n",
    "    # Start taking snapshots only from half the total trees and take every K-th model\n",
    "    start_at = total_trees // 2\n",
    "    ensemble_models = []\n",
    "    for i in range(start_at, total_trees, K):\n",
    "        sub_model = model.copy()\n",
    "        sub_model.shrink(ntree_start=0, ntree_end=i)\n",
    "        ensemble_models.append(sub_model)\n",
    "    return ensemble_models\n",
    "\n",
    "\n",
    "# Function to predict with uncertainty\n",
    "def predict_with_uncertainty(ensemble_models, X):\n",
    "    predictions = np.array([model.predict_proba(X)[:, 1] for model in ensemble_models])\n",
    "    mean_predictions = np.mean(predictions, axis=0)\n",
    "    std_dev_predictions = np.std(predictions, axis=0)\n",
    "    \n",
    "    return mean_predictions, std_dev_predictions\n",
    "\n",
    "\n",
    "def predict_and_explain(model_prom, model_det, df, features_dummy, K_uncertainty):\n",
    "    \"\"\"\n",
    "    Realiza predicciones y genera explicaciones para modelos de promotores y detractores\n",
    "    para todo el dataframe.\n",
    "\n",
    "    Args:\n",
    "    - model_prom: Modelo entrenado para predecir promotores.\n",
    "    - model_det: Modelo entrenado para predecir detractores.\n",
    "    - df: DataFrame con los datos.\n",
    "    - features_dummy: Lista de características utilizadas para las predicciones.\n",
    "\n",
    "    Returns:\n",
    "    - Df final con .data, .values, .base_value, y predicciones.\n",
    "    \"\"\"\n",
    "    # 1. Asumiendo que las funciones de cálculo de SHAP y probabilidad ya están implementadas y ajustadas para usar df\n",
    "    df_contrib = calculate_SHAP_and_probability_binary(model_prom, model_det, df)\n",
    "\n",
    "    # 3. Convertir valores SHAP a probabilidad\n",
    "    df_probability_prom = from_shap_to_probability_binary(df_contrib, features_dummy, 'promoter_binary')\n",
    "    df_probability_det = from_shap_to_probability_binary(df_contrib, features_dummy, 'detractor_binary')\n",
    "    \n",
    "    # 3.5 Calcular incertidumbre para ambos models\n",
    "    # Generate virtual ensemble\n",
    "    total_trees_prom = model_prom.tree_count_  # or any predefined number if you already know the model's tree count\n",
    "    total_trees_det = model_det.tree_count_\n",
    "    virtual_ensemble_prom = create_virtual_ensemble(model_prom, total_trees_prom, K_uncertainty)\n",
    "    virtual_ensemble_det = create_virtual_ensemble(model_det, total_trees_det, K_uncertainty)\n",
    "    \n",
    "    # Use the ensemble to predict on new data\n",
    "    mean_proba_prom, uncertainty_prom = predict_with_uncertainty(virtual_ensemble_prom, df_probability_prom[features_dummy])\n",
    "    mean_proba_det, uncertainty_det = predict_with_uncertainty(virtual_ensemble_det, df_probability_det[features_dummy])\n",
    "    \n",
    "    # Add the mean prediction probabilities and uncertainty to the original DataFrame\n",
    "    df_probability_prom['mean_proba_prom'] = mean_proba_prom\n",
    "    df_probability_prom['uncertainty_prom'] = uncertainty_prom\n",
    "\n",
    "    df_probability_det['mean_proba_det'] = mean_proba_det\n",
    "    df_probability_det['uncertainty_det'] = uncertainty_det    \n",
    "    \n",
    "\n",
    "    # 4. Concatenar DataFrames para ambos modelos\n",
    "    df_probability_prom = df_probability_prom.reset_index(drop=True)\n",
    "    df_probability_det = df_probability_det.reset_index(drop=True)\n",
    "    unique_columns_det = [col for col in df_probability_det.columns if col not in df_probability_prom.columns]\n",
    "    df_probability_binary = pd.concat([df_probability_prom, df_probability_det[unique_columns_det]], axis=1)\n",
    "\n",
    "    # 5. Calcular columnas NPS con la diferencia entre _prom y _det\n",
    "    for column in df_probability_binary.columns:\n",
    "        if '_prom' in column:\n",
    "            base_name = column.split('_prom')[0]\n",
    "            det_column = f'{base_name}_det'\n",
    "            if det_column in df_probability_binary.columns:\n",
    "                nps_column = f'{base_name}_nps'\n",
    "                if base_name == 'uncertainty':\n",
    "                    df_probability_binary[nps_column] = df_probability_binary[column] + df_probability_binary[det_column]\n",
    "                else:\n",
    "                    df_probability_binary[nps_column] = df_probability_binary[column] - df_probability_binary[det_column]                \n",
    "                    \n",
    "    return df_probability_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135aed4-7451-47af-ad5e-42503e7b4c3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476191ed-8639-4d2b-9372-6c165bcda00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def elbow_study(df, variables, max_k=15):\n",
    "    \"\"\"\n",
    "    Realiza el estudio del método del codo para determinar el número óptimo de clusters.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con los datos de entrada.\n",
    "        variables (list): Lista de variables a utilizar para clustering.\n",
    "        max_k (int): Número máximo de clusters a evaluar. Default es 15.\n",
    "    \n",
    "    Returns:\n",
    "        list: Inercia para cada número de clusters en el rango de 1 a max_k.\n",
    "    \"\"\"\n",
    "    # Selección de las variables y preprocesamiento\n",
    "    X = df[variables]\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_imputed)\n",
    "    \n",
    "    # Rango de posibles números de clusters\n",
    "    k_range = range(1, max_k + 1)\n",
    "    inertia = []\n",
    "    \n",
    "    # Aplicar KMeans y calcular la inercia para cada k\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Graficar el método del codo\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(k_range, inertia, marker='o')\n",
    "    plt.title('Elbow Method for Optimal k')\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.show()\n",
    "    \n",
    "    return inertia\n",
    "\n",
    "def train_kmeans(df, variables, n_clusters=3):\n",
    "    \"\"\"\n",
    "    Entrena un modelo KMeans en el DataFrame proporcionado.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame con los datos de entrada.\n",
    "        variables (list): Lista de variables a utilizar para clustering.\n",
    "        n_clusters (int): Número de clusters a usar en KMeans.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame con una columna adicional 'cluster' que indica el cluster asignado a cada punto.\n",
    "    \"\"\"\n",
    "    # Selección de las variables y preprocesamiento\n",
    "    X = df[variables]\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_imputed)\n",
    "    \n",
    "    # Entrenar KMeans con el número especificado de clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def classify_into_clusters(df, touchpoints, scaler, kmeans):\n",
    "    \"\"\"\n",
    "    Aplica el modelo K-means entrenado a un dataframe dado y asigna las etiquetas de cluster.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame con los datos a segmentar.\n",
    "    touchpoints (list): Lista de columnas de touchpoints.\n",
    "    scaler (StandardScaler): Objeto StandardScaler ya entrenado.\n",
    "    kmeans (KMeans): Objeto KMeans ya entrenado.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame original con las etiquetas de cluster asignadas.\n",
    "    \"\"\"\n",
    "    # Copiar el dataframe original para evitar modificaciones accidentales\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Filtrar el dataframe\n",
    "    filtered_df = df_copy[touchpoints]\n",
    "\n",
    "    # Reemplazar NaNs con -1 (u otro valor que no interfiera en tu análisis)\n",
    "    filtered_df.fillna(-1, inplace=True)\n",
    "\n",
    "    # Normalización de las variables utilizando el mismo scaler entrenado\n",
    "    X_new_scaled = scaler.transform(filtered_df)\n",
    "\n",
    "    # Aplicar el modelo K-means entrenado al nuevo dataframe\n",
    "    new_clusters = kmeans.predict(X_new_scaled)\n",
    "\n",
    "    # Asignar las etiquetas de cluster al dataframe original\n",
    "    df_copy['cluster'] = new_clusters\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e55481-6373-4fd5-9d41-b54671e7572a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simualtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664ced7e-a9b6-4c53-be0c-33c371b72aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_total_distance(df, targets, touchpoints):\n",
    "    \"\"\"\n",
    "    Calcula la distancia total entre las satisfacciones actuales y los targets utilizando pesos suaves.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame con las puntuaciones de los clientes.\n",
    "    targets (pd.DataFrame): DataFrame con una fila que contiene los targets de satisfacción.\n",
    "    touchpoints (list): Lista de touchpoints a considerar.\n",
    "    \n",
    "    Returns:\n",
    "    float: Distancia total ponderada.\n",
    "    \"\"\"\n",
    "    current_satisfaction = {f'{tp}_satisfaction': calculate_satisfaction(df, tp) for tp in touchpoints}\n",
    "    total_distance = 0.0\n",
    "    \n",
    "    for tp in touchpoints:\n",
    "        target_value = targets[f'{tp}_satisfaction'].mean()\n",
    "        current_value = current_satisfaction[f'{tp}_satisfaction']\n",
    "        if not np.isnan(current_value):\n",
    "            total_distance += euclidean([target_value], [current_value])  \n",
    "    \n",
    "    return total_distance\n",
    "\n",
    "import time\n",
    "\n",
    "def soft_manual_sim_causal(df, targets, touchpoints, df_original, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Adjusts customer scores in the DataFrame to meet satisfaction targets.\n",
    "    Ensures final satisfactions are within desired intervals based on the necessary change.\n",
    "    Args:\n",
    "        df (pd.DataFrame): Adjusted DataFrame after previous modifications.\n",
    "        targets (pd.DataFrame): DataFrame with satisfaction targets.\n",
    "        touchpoints (list): List of touchpoints to adjust.\n",
    "        df_original (pd.DataFrame): Original DataFrame before any adjustments.\n",
    "        threshold (float): Threshold to consider when adjustments have reached the target.\n",
    "    Returns:\n",
    "        pd.DataFrame: Adjusted DataFrame.\n",
    "    \"\"\"\n",
    "    df_adjusted = df.copy()\n",
    "    adjusted_clients_count = 0\n",
    "    sum_differences = 0  # To store the sum of differences\n",
    "\n",
    "    # Track the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Calculate original satisfactions from the original DataFrame\n",
    "    original_satisfactions = {}\n",
    "    for variable in targets.columns:\n",
    "        target = targets[variable].values[0]\n",
    "        if variable.endswith('_satisfaction'):\n",
    "            touchpoint = variable.replace('_satisfaction', '')\n",
    "            if touchpoint in touchpoints:\n",
    "                original_satisfaction = calculate_satisfaction(df_original, touchpoint)\n",
    "                original_satisfactions[variable] = original_satisfaction\n",
    "                sum_differences += original_satisfaction - target\n",
    "\n",
    "    for variable in targets.columns:\n",
    "        # Break if the execution time exceeds 10 minutes\n",
    "        if time.time() - start_time > 300:\n",
    "            print(\"Execution stopped: exceeded time limit of 10 minutes.\")\n",
    "            break\n",
    "\n",
    "        target = targets[variable].values[0]\n",
    "        print(variable, target)\n",
    "        if variable.endswith('_satisfaction'):\n",
    "            touchpoint = variable.replace('_satisfaction', '')\n",
    "            if touchpoint in touchpoints:\n",
    "                current_satisfaction = calculate_satisfaction(df_adjusted, touchpoint)\n",
    "                original_satisfaction = original_satisfactions[variable]\n",
    "\n",
    "                # Determine desired interval based on whether satisfaction needed to increase or decrease\n",
    "                if original_satisfaction < target:\n",
    "                    # Satisfaction needed to increase\n",
    "                    lower_bound = target\n",
    "                    upper_bound = target + threshold\n",
    "                elif original_satisfaction > target:\n",
    "                    # Satisfaction needed to decrease\n",
    "                    lower_bound = target - threshold\n",
    "                    upper_bound = target\n",
    "                else:\n",
    "                    # Satisfaction was equal to target\n",
    "                    if sum_differences > 0:\n",
    "                        # Adjust upwards\n",
    "                        lower_bound = original_satisfaction\n",
    "                        upper_bound = original_satisfaction + threshold\n",
    "                    else:\n",
    "                        # Adjust downwards\n",
    "                        lower_bound = original_satisfaction - threshold\n",
    "                        upper_bound = original_satisfaction\n",
    "\n",
    "                # Adjust satisfaction upwards\n",
    "                if current_satisfaction < lower_bound:\n",
    "                    for value in [7, 6, 5, 4, 3, 2, 1]:\n",
    "                        while current_satisfaction < lower_bound:\n",
    "                            # Break if the execution time exceeds 10 minutes\n",
    "                            if time.time() - start_time > 600:\n",
    "                                print(\"Execution stopped: exceeded time limit of 10 minutes.\")\n",
    "                                break\n",
    "\n",
    "                            to_adjust = df_adjusted[df_adjusted[touchpoint] == value]\n",
    "                            if to_adjust.empty:\n",
    "                                break\n",
    "                            to_adjust_sample = to_adjust.sample(n=1)\n",
    "                            df_adjusted.loc[to_adjust_sample.index, touchpoint] = 8\n",
    "                            adjusted_clients_count += 1\n",
    "                            current_satisfaction = calculate_satisfaction(df_adjusted, touchpoint)\n",
    "                            if current_satisfaction >= lower_bound:\n",
    "                                break\n",
    "                        if current_satisfaction >= lower_bound:\n",
    "                            break\n",
    "\n",
    "                # Adjust satisfaction downwards\n",
    "                elif current_satisfaction > upper_bound:\n",
    "                    for value in [8, 9, 10]:\n",
    "                        while current_satisfaction > upper_bound:\n",
    "                            # Break if the execution time exceeds 10 minutes\n",
    "                            if time.time() - start_time > 600:\n",
    "                                print(\"Execution stopped: exceeded time limit of 10 minutes.\")\n",
    "                                break\n",
    "\n",
    "                            to_adjust = df_adjusted[df_adjusted[touchpoint] == value]\n",
    "                            if to_adjust.empty:\n",
    "                                break\n",
    "                            to_adjust_sample = to_adjust.sample(n=1)\n",
    "                            df_adjusted.loc[to_adjust_sample.index, touchpoint] = 7\n",
    "                            adjusted_clients_count += 1\n",
    "                            current_satisfaction = calculate_satisfaction(df_adjusted, touchpoint)\n",
    "                            if current_satisfaction <= upper_bound:\n",
    "                                break\n",
    "                        if current_satisfaction <= upper_bound:\n",
    "                            break\n",
    "\n",
    "                else:\n",
    "                    print(f\"Current satisfaction for {touchpoint} is within the desired interval.\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Touchpoint {touchpoint} not in touchpoints list.\")\n",
    "\n",
    "        else:\n",
    "            # For non-satisfaction variables (e.g., mean scores)\n",
    "            current_mean = calculate_mean(df_adjusted, variable)\n",
    "            original_mean = calculate_mean(df_original, variable)\n",
    "            if original_mean < target:\n",
    "                desired_range = (target, target + threshold)\n",
    "            elif original_mean > target:\n",
    "                desired_range = (target - threshold, target)\n",
    "            else:\n",
    "                if sum_differences > 0:\n",
    "                    desired_range = (original_mean, original_mean + threshold)\n",
    "                else:\n",
    "                    desired_range = (original_mean - threshold, original_mean)\n",
    "\n",
    "            if current_mean < desired_range[0]:\n",
    "                while current_mean < desired_range[0]:\n",
    "                    # Break if the execution time exceeds 10 minutes\n",
    "                    if time.time() - start_time > 600:\n",
    "                        print(\"Execution stopped: exceeded time limit of 10 minutes.\")\n",
    "                        break\n",
    "\n",
    "                    n = desired_range[0] - current_mean\n",
    "                    adjustment_needed = min(n, threshold)\n",
    "                    df_adjusted[variable] += adjustment_needed\n",
    "                    adjusted_clients_count += 1\n",
    "                    current_mean = calculate_mean(df_adjusted, variable)\n",
    "            elif current_mean > desired_range[1]:\n",
    "                while current_mean > desired_range[1]:\n",
    "                    # Break if the execution time exceeds 10 minutes\n",
    "                    if time.time() - start_time > 600:\n",
    "                        print(\"Execution stopped: exceeded time limit of 10 minutes.\")\n",
    "                        break\n",
    "\n",
    "                    n = current_mean - desired_range[1]\n",
    "                    adjustment_needed = min(n, threshold)\n",
    "                    df_adjusted[variable] -= adjustment_needed\n",
    "                    adjusted_clients_count += 1\n",
    "                    current_mean = calculate_mean(df_adjusted, variable)\n",
    "            else:\n",
    "                print(f\"Current mean for {variable} is within the desired interval.\")\n",
    "\n",
    "        print(f\"Total clients adjusted: {adjusted_clients_count}\")\n",
    "\n",
    "    return df_adjusted\n",
    "\n",
    "\n",
    "\n",
    "def hard_manual_sim_rand_cluster_causal(df, targets, touchpoints, df_original, threshold=0.05, num_clients_per_iter=1):\n",
    "    \"\"\"\n",
    "    Adjusts customer scores in the DataFrame to meet satisfaction targets\n",
    "    by changing multiple touchpoints for a group of clients in each iteration.\n",
    "    Ensures final satisfactions are within desired intervals based on the necessary change.\n",
    "    Args:\n",
    "        df (pd.DataFrame): Adjusted DataFrame after previous modifications.\n",
    "        targets (pd.DataFrame): DataFrame with satisfaction targets.\n",
    "        touchpoints (list): List of touchpoints to adjust.\n",
    "        df_original (pd.DataFrame): Original DataFrame before any adjustments.\n",
    "        threshold (float): Threshold to consider when adjustments have reached the target.\n",
    "        num_clients_per_iter (int): Number of clients to select in each iteration.\n",
    "    Returns:\n",
    "        pd.DataFrame: Adjusted DataFrame.\n",
    "    \"\"\"\n",
    "    df_adjusted = df.copy()\n",
    "    df_adjusted['flag_reached'] = 0\n",
    "    selected_clients = set()\n",
    "\n",
    "    variables_to_increase = []\n",
    "    variables_to_decrease = []\n",
    "\n",
    "    adjusted_clients_count = 0\n",
    "    sum_differences = 0  # To store the sum of differences\n",
    "    original_satisfactions = {}\n",
    "\n",
    "    # Calculate original satisfactions from the original DataFrame\n",
    "    for variable in targets.columns:\n",
    "        target = targets[variable].values[0]\n",
    "\n",
    "        if variable.endswith('_satisfaction'):\n",
    "            touchpoint = variable.replace('_satisfaction', '')\n",
    "            if touchpoint in touchpoints:\n",
    "                original_satisfaction = calculate_satisfaction(df_original, touchpoint)\n",
    "                original_satisfactions[variable] = original_satisfaction\n",
    "                sum_differences += original_satisfaction - target\n",
    "\n",
    "                if original_satisfaction < target:\n",
    "                    variables_to_increase.append((touchpoint, target))\n",
    "                elif original_satisfaction > target:\n",
    "                    variables_to_decrease.append((touchpoint, target))\n",
    "                else:\n",
    "                    # Satisfaction was equal to target\n",
    "                    if sum_differences > 0:\n",
    "                        # Need to increase within [original, original + threshold]\n",
    "                        variables_to_increase.append((touchpoint, original_satisfaction + threshold))\n",
    "                    else:\n",
    "                        # Need to decrease within [original - threshold, original]\n",
    "                        variables_to_decrease.append((touchpoint, original_satisfaction - threshold))\n",
    "                        \n",
    "    # Ordenar las variables según el orden en touchpoints\n",
    "    variables_to_increase.sort(key=lambda x: touchpoints.index(x[0]))\n",
    "    variables_to_decrease.sort(key=lambda x: touchpoints.index(x[0]))\n",
    "    \n",
    "    # Increase scores\n",
    "    while variables_to_increase:\n",
    "        # if not df_adjusted[df_adjusted['cluster'] == 2].empty:\n",
    "        #     available_indices = df_adjusted[df_adjusted['cluster'] == 2].index.difference(selected_clients)\n",
    "        #     if not available_indices.empty:\n",
    "        #         to_adjust_samples = df_adjusted.loc[available_indices].sample(n=min(num_clients_per_iter, len(available_indices)))\n",
    "        #     else:\n",
    "        #         to_adjust_samples = df_adjusted.sample(n=num_clients_per_iter)\n",
    "        # else:\n",
    "        #     to_adjust_samples = df_adjusted.sample(n=num_clients_per_iter)\n",
    "            \n",
    "        available_indices = df_adjusted.index.difference(selected_clients)\n",
    "\n",
    "        # Seleccionar un grupo de clientes que no hayan sido seleccionados previamente\n",
    "        to_adjust_samples = df_adjusted.loc[available_indices].sample(n=min(num_clients_per_iter, len(available_indices)))\n",
    "            \n",
    "        \n",
    "\n",
    "        selected_clients.update(to_adjust_samples.index)\n",
    "\n",
    "        for value in [7, 6, 5, 4, 3, 2, 1, 0]:\n",
    "            adjusted = False\n",
    "            for touchpoint, target in variables_to_increase.copy():\n",
    "                current_satisfaction = calculate_satisfaction(df_adjusted, touchpoint)\n",
    "                if current_satisfaction >= target:\n",
    "                    print(f\"Variable {touchpoint} has reached the target satisfaction.\")\n",
    "                    variables_to_increase.remove((touchpoint, target))\n",
    "                    continue\n",
    "\n",
    "                mask = (to_adjust_samples[touchpoint] == value) | (to_adjust_samples[touchpoint] == value - 1)\n",
    "                if mask.any():\n",
    "                    new_value = random.randint(8, 10)\n",
    "                    df_adjusted.loc[to_adjust_samples[mask].index, touchpoint] = new_value\n",
    "                    adjusted = True\n",
    "            if adjusted:\n",
    "                adjusted_clients_count += len(to_adjust_samples[mask])\n",
    "                break\n",
    "\n",
    "    # Decrease scores\n",
    "    while variables_to_decrease:\n",
    "        # if not df_adjusted[df_adjusted['cluster'] == 1].empty:\n",
    "        #     available_indices = df_adjusted[df_adjusted['cluster'] == 1].index.difference(selected_clients)\n",
    "        #     if not available_indices.empty:\n",
    "        #         to_adjust_samples = df_adjusted.loc[available_indices].sample(n=min(num_clients_per_iter, len(available_indices)))\n",
    "        #     else:\n",
    "        #         to_adjust_samples = df_adjusted.sample(n=num_clients_per_iter)\n",
    "        # else:\n",
    "        #     to_adjust_samples = df_adjusted.sample(n=num_clients_per_iter)\n",
    "        \n",
    "        available_indices = df_adjusted.index.difference(selected_clients)\n",
    "        # Seleccionar un grupo de clientes que no hayan sido seleccionados previamente\n",
    "        to_adjust_samples = df_adjusted.loc[available_indices].sample(n=min(num_clients_per_iter, len(available_indices)))\n",
    "\n",
    "        selected_clients.update(to_adjust_samples.index)\n",
    "        \n",
    "\n",
    "        for value in [8, 9, 10]:\n",
    "            adjusted = False\n",
    "            for touchpoint, target in variables_to_decrease.copy():\n",
    "                current_satisfaction = calculate_satisfaction(df_adjusted, touchpoint)\n",
    "                if current_satisfaction <= target:\n",
    "                    print(f\"Variable {touchpoint} has reached the target satisfaction.\")\n",
    "                    variables_to_decrease.remove((touchpoint, target))\n",
    "                    continue\n",
    "\n",
    "                mask = (to_adjust_samples[touchpoint] == value) | (to_adjust_samples[touchpoint] == value + 1)\n",
    "                if mask.any():\n",
    "                    new_value = random.randint(0, 7)\n",
    "                    df_adjusted.loc[to_adjust_samples[mask].index, touchpoint] = new_value\n",
    "                    adjusted = True\n",
    "            if adjusted:\n",
    "                adjusted_clients_count += len(to_adjust_samples[mask])\n",
    "                break\n",
    "\n",
    "    print(f\"Total clients adjusted: {adjusted_clients_count}\")\n",
    "\n",
    "    # Final verification\n",
    "    verification_results = {}\n",
    "    for variable in targets.columns:\n",
    "        if variable.endswith('_satisfaction'):\n",
    "            touchpoint = variable.replace('_satisfaction', '')\n",
    "            if touchpoint in touchpoints:\n",
    "                final_satisfaction = calculate_satisfaction(df_adjusted, touchpoint)\n",
    "                verification_results[variable] = (final_satisfaction, targets[variable].values[0])\n",
    "        else:\n",
    "            final_mean = calculate_mean(df_adjusted, variable)\n",
    "            verification_results[variable] = (final_mean, targets[variable].values[0])\n",
    "\n",
    "    for var, (final, target) in verification_results.items():\n",
    "        print(f\"Variable: {var}, Final: {final:.2f}, Target: {target:.2f}, Met: {abs(final - target) <= threshold}\")\n",
    "\n",
    "    return df_adjusted\n",
    "\n",
    "# Crear un diccionario de pesos basado en el orden de importancia, usando una función logarítmica\n",
    "def log_weight_function(index, max_index):\n",
    "    return 1 - np.log(1 + index) / np.log(1 + max_index)\n",
    "\n",
    "def causal_swapping(df, targets, touchpoints, df_original, max_iterations=100000, threshold=0.05, patience=500):\n",
    "    \"\"\"\n",
    "    Adjusts the customer population by swapping clients between clusters until targets are met or no further improvement.\n",
    "    Ensures final satisfactions are within desired intervals based on the necessary change.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with customer scores.\n",
    "        targets (pd.DataFrame): DataFrame with satisfaction targets.\n",
    "        touchpoints (list): List of touchpoints to adjust.\n",
    "        df_original (pd.DataFrame): Original DataFrame before any adjustments.\n",
    "        max_iterations (int): Maximum number of iterations.\n",
    "        threshold (float): Threshold to consider when adjustments have reached the target.\n",
    "        patience (int): Number of additional iterations after the distance stops improving.\n",
    "    Returns:\n",
    "        pd.DataFrame: Adjusted DataFrame.\n",
    "    \"\"\"\n",
    "    df_adjusted = df.copy()\n",
    "\n",
    "    # Calculate original satisfactions and initial values\n",
    "    original_satisfactions = {tp: calculate_satisfaction(df_original, tp) for tp in touchpoints}\n",
    "    overall_satisfaction = {tp: calculate_satisfaction(df_adjusted, tp) for tp in touchpoints}\n",
    "    initial_distance = calculate_total_distance(df_adjusted, targets, touchpoints)\n",
    "    iteration = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        adjustments_made = False\n",
    "        local_adjustments_made = False\n",
    "\n",
    "        # Loop through touchpoints to adjust satisfactions\n",
    "        for tp in touchpoints:\n",
    "            target_value = targets[f'{tp}_satisfaction'].values[0]\n",
    "            current_satisfaction = overall_satisfaction[tp]\n",
    "            original_satisfaction = original_satisfactions[tp]\n",
    "\n",
    "            # Define desired interval\n",
    "            if original_satisfaction < target_value:\n",
    "                lower_bound, upper_bound = target_value, target_value + threshold\n",
    "            elif original_satisfaction > target_value:\n",
    "                lower_bound, upper_bound = target_value - threshold, target_value\n",
    "            else:\n",
    "                lower_bound, upper_bound = original_satisfaction - threshold, original_satisfaction + threshold\n",
    "\n",
    "            # Check if adjustment is needed\n",
    "            if current_satisfaction < lower_bound:\n",
    "                # Need to increase satisfaction by swapping from cluster 2 to cluster 0 with specific logic\n",
    "                df_adjusted, local_adjustments_made = swap_client_with_criteria(df_adjusted, from_cluster=2, to_cluster=0, touchpoint=tp, increase=True)\n",
    "            elif current_satisfaction > upper_bound:\n",
    "                # Need to decrease satisfaction by swapping from cluster 0 to cluster 2 with opposite specific logic\n",
    "                df_adjusted, local_adjustments_made = swap_client_with_criteria(df_adjusted, from_cluster=0, to_cluster=2, touchpoint=tp, increase=False)\n",
    "\n",
    "            # Break the loop if adjustment was made\n",
    "            if local_adjustments_made:\n",
    "                # Recalculate distances and satisfaction after adjustments\n",
    "                new_distance = calculate_total_distance(df_adjusted, targets, touchpoints)\n",
    "                overall_satisfaction = {tp: calculate_satisfaction(df_adjusted, tp) for tp in touchpoints}\n",
    "\n",
    "                # If new distance is better, reset patience counter and break\n",
    "                if new_distance < initial_distance:\n",
    "                    initial_distance = new_distance\n",
    "                    patience_counter = 0\n",
    "                    adjustments_made = True\n",
    "                    break  # Successful adjustment, move to next iteration of while loop\n",
    "                else:\n",
    "                    # No improvement, continue with next touchpoint\n",
    "                    break\n",
    "\n",
    "        # If no adjustments were made in the entire loop through touchpoints, apply simple swap\n",
    "        if not adjustments_made:\n",
    "            simple_adjustment_made = simple_swap_clients_between_clusters(df_adjusted, from_cluster=2, to_cluster=0)\n",
    "            if simple_adjustment_made:\n",
    "                # Recalculate distances and satisfaction after simple swap\n",
    "                new_distance = calculate_total_distance(df_adjusted, targets, touchpoints)\n",
    "                overall_satisfaction = {tp: calculate_satisfaction(df_adjusted, tp) for tp in touchpoints}\n",
    "\n",
    "                # If new distance is better, reset patience counter\n",
    "                if new_distance < initial_distance:\n",
    "                    initial_distance = new_distance\n",
    "                    patience_counter = 0\n",
    "                    adjustments_made = True\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Local minimum reached at iteration {iteration} with distance {initial_distance:.4f} and patience count {patience_counter}\")\n",
    "                break\n",
    "\n",
    "        # Check if all touchpoints are within desired intervals\n",
    "        if all(within_interval(overall_satisfaction[tp], original_satisfactions[tp], target_value, threshold, sum_differences=sum(target_value - original_satisfaction for tp in touchpoints)) for tp in touchpoints):\n",
    "            print(f\"Targets achieved within desired intervals at iteration {iteration} with distance {new_distance:.4f}\")\n",
    "            break\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return df_adjusted\n",
    "\n",
    "def swap_client_with_criteria(df_adjusted, from_cluster, to_cluster, touchpoint, increase=True):\n",
    "    \"\"\"\n",
    "    Helper function to swap clients between clusters with specific criteria.\n",
    "    Args:\n",
    "        df_adjusted (pd.DataFrame): Adjusted DataFrame.\n",
    "        from_cluster (int): Cluster to remove clients from.\n",
    "        to_cluster (int): Cluster to add clients to.\n",
    "        touchpoints (list): List of touchpoints to consider.\n",
    "        increase (bool): Whether we are increasing or decreasing satisfaction.\n",
    "    Returns:\n",
    "        (pd.DataFrame, bool): Tuple containing the adjusted DataFrame and a boolean indicating if the swap was successful.\n",
    "    \"\"\"\n",
    "    clients_from = df_adjusted[df_adjusted['cluster'] == from_cluster]\n",
    "    clients_to = df_adjusted[df_adjusted['cluster'] == to_cluster]\n",
    "\n",
    "    # Select the most important touchpoint\n",
    "    important_touchpoint = touchpoint\n",
    "\n",
    "    if not clients_from.empty and not clients_to.empty:\n",
    "        if increase:\n",
    "            # Select clients from 'from_cluster' with high out_prob_nps and high contributions in the most important touchpoint\n",
    "            condition_from = (clients_from['out_prob_nps'] > clients_from['out_prob_nps'].quantile(0.90))\n",
    "            condition_from &= (clients_from[f'{important_touchpoint}_nps'] > clients_from[f'{important_touchpoint}_nps'].quantile(0.90))\n",
    "            clients_from_filtered = clients_from[condition_from]\n",
    "\n",
    "            if clients_from_filtered.empty:\n",
    "                return df_adjusted, False\n",
    "\n",
    "            # Select a client to duplicate from 'from_cluster'\n",
    "            client_to_duplicate = clients_from_filtered.sample(n=1)\n",
    "            df_adjusted = df_adjusted.append(client_to_duplicate, ignore_index=True)\n",
    "\n",
    "            # Select clients from 'to_cluster' with low out_prob_nps and negative contributions in the most important touchpoint\n",
    "            condition_to = (clients_to['out_prob_nps'] < clients_to['out_prob_nps'].quantile(0.10))\n",
    "            condition_to &= (clients_to[f'{important_touchpoint}_nps'] < clients_to[f'{important_touchpoint}_nps'].quantile(0.10))\n",
    "            clients_to_filtered = clients_to[condition_to]\n",
    "\n",
    "            if clients_to_filtered.empty:\n",
    "                return df_adjusted, False\n",
    "\n",
    "            # Select a client to remove from 'to_cluster'\n",
    "            client_to_remove = clients_to_filtered.sample(n=1)\n",
    "            df_adjusted.drop(client_to_remove.index, inplace=True)\n",
    "        else:\n",
    "            # Decrease satisfaction: Opposite logic\n",
    "            # Select clients from 'from_cluster' with low out_prob_nps and negative contributions in the most important touchpoint\n",
    "            condition_from = (clients_from['out_prob_nps'] < clients_from['out_prob_nps'].quantile(0.10))\n",
    "            condition_from &= (clients_from[f'{important_touchpoint}_nps'] < clients_from[f'{important_touchpoint}_nps'].quantile(0.10))\n",
    "            clients_from_filtered = clients_from[condition_from]\n",
    "\n",
    "            if clients_from_filtered.empty:\n",
    "                return df_adjusted, False\n",
    "\n",
    "            # Select a client to duplicate from 'from_cluster'\n",
    "            client_to_duplicate = clients_from_filtered.sample(n=1)\n",
    "            df_adjusted = df_adjusted.append(client_to_duplicate, ignore_index=True)\n",
    "\n",
    "            # Select clients from 'to_cluster' with high out_prob_nps and positive contributions in the most important touchpoint\n",
    "            condition_to = (clients_to['out_prob_nps'] > clients_to['out_prob_nps'].quantile(0.90))\n",
    "            condition_to &= (clients_to[f'{important_touchpoint}_nps'] > clients_to[f'{important_touchpoint}_nps'].quantile(0.90))\n",
    "            clients_to_filtered = clients_to[condition_to]\n",
    "\n",
    "            if clients_to_filtered.empty:\n",
    "                return df_adjusted, False\n",
    "\n",
    "            # Select a client to remove from 'to_cluster'\n",
    "            client_to_remove = clients_to_filtered.sample(n=1)\n",
    "            df_adjusted.drop(client_to_remove.index, inplace=True)\n",
    "\n",
    "        return df_adjusted.copy(), True\n",
    "    return df_adjusted, False\n",
    "\n",
    "def simple_swap_clients_between_clusters(df_adjusted, from_cluster, to_cluster):\n",
    "    \"\"\"\n",
    "    Helper function to simply swap clients between clusters.\n",
    "    Args:\n",
    "        df_adjusted (pd.DataFrame): Adjusted DataFrame.\n",
    "        from_cluster (int): Cluster to remove clients from.\n",
    "        to_cluster (int): Cluster to add clients to.\n",
    "    Returns:\n",
    "        bool: True if swap was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    clients_from = df_adjusted[df_adjusted['cluster'] == from_cluster]\n",
    "    clients_to = df_adjusted[df_adjusted['cluster'] == to_cluster]\n",
    "\n",
    "    if not clients_from.empty and not clients_to.empty:\n",
    "        # Select a random client from 'from_cluster'\n",
    "        client_to_remove = clients_from.sample(n=1)\n",
    "        df_adjusted.drop(client_to_remove.index, inplace=True)\n",
    "\n",
    "        # Select a random client from 'to_cluster'\n",
    "        client_to_add = clients_to.sample(n=1)\n",
    "        df_adjusted = pd.concat([df_adjusted, client_to_add], ignore_index=True)\n",
    "\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def within_interval(current, original, target, threshold, sum_differences):\n",
    "    \"\"\"Helper function to determine if satisfaction is within the desired interval.\"\"\"\n",
    "    if original < target:\n",
    "        return target <= current <= target + threshold\n",
    "    elif original > target:\n",
    "        return target - threshold <= current <= target\n",
    "    else:\n",
    "        if sum_differences > 0:\n",
    "            return original <= current <= original + threshold\n",
    "        else:\n",
    "            return original - threshold <= current <= original\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028bb74b-1bb0-4dd9-97b6-78f48d116997",
   "metadata": {
    "tags": []
   },
   "source": [
    "# List of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed941b-e320-4fc9-a3e2-30f62f362442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "op_vars = [\"ticket_price\", \"load_factor\"]\n",
    "\n",
    "touchpoints = [\"pun_100_punctuality\",\n",
    "    \"bkg_200_journey_preparation\",\n",
    "    \"pfl_100_checkin\",\n",
    "    \"pfl_200_security\",\n",
    "    \"pfl_300_lounge\",\n",
    "    \"pfl_500_boarding\",\n",
    "    \"ifl_300_cabin\",\n",
    "    \"ifl_200_flight_crew_annoucements\",\n",
    "    \"ifl_600_wifi\",\n",
    "    \"ifl_500_ife\",\n",
    "    \"ifl_400_food_drink\",\n",
    "    \"ifl_100_cabin_crew\",\n",
    "    \"arr_100_arrivals\",\n",
    "    \"con_100_connections\",\n",
    "    \"loy_200_loyalty_programme\",\n",
    "    \"img_310_ease_contact_phone\"]\n",
    "\n",
    "labels = [\"promoter_binary\",\n",
    "    \"detractor_binary\"]\n",
    "\n",
    "shaps = [\"pun_100_punctuality_nps\",\n",
    "    \"bkg_200_journey_preparation_nps\",\n",
    "    \"pfl_100_checkin_nps\",\n",
    "    \"pfl_200_security_nps\",\n",
    "    \"pfl_300_lounge_nps\",\n",
    "    \"pfl_500_boarding_nps\",\n",
    "    \"ifl_300_cabin_nps\",\n",
    "    \"ifl_200_flight_crew_annoucements_nps\",\n",
    "    \"ifl_600_wifi_nps\",\n",
    "    \"ifl_500_ife_nps\",\n",
    "    \"ifl_400_food_drink_nps\",\n",
    "    \"ifl_100_cabin_crew_nps\",\n",
    "    \"arr_100_arrivals_nps\",\n",
    "    \"con_100_connections_nps\",\n",
    "    \"loy_200_loyalty_programme_nps\",\n",
    "    \"img_310_ease_contact_phone_nps\"]\n",
    "\n",
    "preds = [\"out_prob_prom\",\n",
    "    \"out_prob_det\",\n",
    "    \"out_prob_nps\"]\n",
    "\n",
    "nps = ['nps_100']\n",
    "\n",
    "variables = touchpoints + labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a0688-aa10-4c78-a8be-9dff650211bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read historic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3414bb5-0dad-4ff7-a36b-666563e43872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define the S3 bucket and prefix\n",
    "S3_BUCKET = \"iberia-data-lake\"\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "insert_date_ci='2024-10-28'\n",
    "prefix = f'customer/nps_surveys/export_historic/insert_date_ci={insert_date_ci}/'\n",
    "\n",
    "STR_START_DATE_o = '2023-06-01'\n",
    "STR_END_DATE_o = '2023-08-30'\n",
    "STR_START_DATE_f = '2024-06-01'\n",
    "STR_END_DATE_f = '2024-08-30'\n",
    "STR_CABIN = 'Business'\n",
    "STR_HAUL = 'LH'\n",
    "\n",
    "month = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b403a85f-1fe7-4963-8e50-535cf6f85873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sts_client = boto3.client('sts')\n",
    "\n",
    "assumed_role = sts_client.assume_role(\n",
    "    RoleArn=\"arn:aws:iam::320714865578:role/ibdata-prod-role-assume-customer-services-from-ibdata-aip-prod\",\n",
    "    RoleSessionName=\"test\"\n",
    ")\n",
    "\n",
    "credentials = assumed_role['Credentials']\n",
    "\n",
    "# Configura s3fs para acceder a S3 con tus credenciales\n",
    "fs = s3fs.S3FileSystem(key=credentials['AccessKeyId'], secret=credentials['SecretAccessKey'], token=credentials['SessionToken'])\n",
    "\n",
    "# Especifica la ruta de la carpeta\n",
    "bucket_name = 'ibdata-prod-ew1-s3-customer'\n",
    "folder_path = 'customer/load_factor_to_s3_nps_model/'\n",
    "\n",
    "# Lista todos los archivos en la carpeta\n",
    "files = fs.ls(f'{bucket_name}/{folder_path}')\n",
    "\n",
    "# Leer y concatenar todos los archivos Parquet en un solo DataFrame\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    with fs.open(f's3://{file}') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concat the dfs\n",
    "df_lf_historic = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95665585-d5bf-4c4a-8b6e-f9fa0ea2c492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_keys = [item.key for item in s3_resource.Bucket(S3_BUCKET).objects.filter(Prefix=prefix)]\n",
    "preprocess_paths = [f\"s3://{S3_BUCKET}/{key}\" for key in s3_keys]\n",
    "df_nps_historic = pd.DataFrame()\n",
    "for file in preprocess_paths:\n",
    "    df = pd.read_csv(file)\n",
    "    df_nps_historic = pd.concat([df_nps_historic, df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0c919-06d1-484e-9500-f3d5a068004b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b615d7ef-db42-4f1b-b920-af5444cbdf5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764b1d4-033d-4291-90e0-2338b4e6b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3 object\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "\n",
    "# path\n",
    "model_path = f\"customer/simulations/sbx/01_clusterize_step/model/3means.pkl\"\n",
    "scaler_path = f\"customer/simulations/sbx/01_clusterize_step/model/3means_scaler.pkl\"\n",
    "        \n",
    "# Load the trained model from S3\n",
    "model = (\n",
    "    s3_resource.Bucket(S3_BUCKET)\n",
    "    .Object(f\"{model_path}\")\n",
    "    .get()\n",
    ")\n",
    "kmeans_model = pickle.loads(model[\"Body\"].read())      \n",
    "    \n",
    "scaler = (\n",
    "    s3_resource.Bucket(S3_BUCKET)\n",
    "    .Object(f\"{scaler_path}\")\n",
    "    .get()\n",
    ")\n",
    "kmeans_scaler = pickle.loads(scaler[\"Body\"].read())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef00b2f-e9c3-4c83-bcc9-a575bf028fd9",
   "metadata": {},
   "source": [
    "## Classifiers (Client Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea0fec-38d8-4c87-9d78-5bbb0861baca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_names = ['Promoters', 'Detractors']\n",
    "\n",
    "# Define the paths for reading data and the trained model\n",
    "clf_model={}\n",
    "for name in model_names:\n",
    "    path_read_train = f\"customer/nps_explainability_model/prod/02_train_step/{name}\"\n",
    "\n",
    "    # Determine the path to read the model from\n",
    "    model_path, model_year, model_month, model_day = get_path_to_read_and_date(\n",
    "        read_last_date=True,\n",
    "        bucket=S3_BUCKET,\n",
    "        key=path_read_train,\n",
    "        partition_date=insert_date_ci,\n",
    "    )\n",
    "\n",
    "    # Extract the bucket and object key from the model_path\n",
    "    if 's3://' in model_path:\n",
    "        model_path = model_path.split('//')[1].replace(f\"{S3_BUCKET}/\", '')\n",
    "\n",
    "    # Load the trained model from S3\n",
    "    fitted_clf_model = (\n",
    "        s3_resource.Bucket(S3_BUCKET)\n",
    "        .Object(f\"{model_path}/model/CatBoostClassifier_cv.pkl\")\n",
    "        .get()\n",
    "    )\n",
    "    clf_model[name] = pickle.loads(fitted_clf_model[\"Body\"].read())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d802f38b-e628-48a4-870f-75fd918d5e72",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pickle\n",
    "\n",
    "# Definir los nombres de los modelos y las rutas de los archivos\n",
    "model_names = ['Promoters', 'Detractors']\n",
    "model_paths = ['model_prom.pkl', 'model_det.pkl']\n",
    "\n",
    "# Cargar los modelos en el diccionario clf_model\n",
    "clf_model = {}\n",
    "for name, path in zip(model_names, model_paths):\n",
    "    with open(path, 'rb') as file:\n",
    "        clf_model[name] = pickle.load(file)\n",
    "\n",
    "# Verificar la carga\n",
    "print(\"Modelos cargados:\", clf_model.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08e0ca-5e47-409c-90b2-c61b335ed433",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clean and filter df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099a94e-89fb-4643-9e5d-7cf22c1d7a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # ETL Code\n",
    "\n",
    "# 1. Filter dataframes by carrier code.\n",
    "df_nps_historic['haul'] = df_nps_historic['haul'].replace('MH', 'SH')\n",
    "    \n",
    "# NPS HISTORIC\n",
    "condition_1 = (df_nps_historic['operating_airline_code'].isin(['IB', 'YW']))\n",
    "# condition_1 = (df_nps_historic['operating_airline_code'].isin(['IB'])) #Ejercicio FP de Iberia por separado\n",
    "condition_2 = ((df_nps_historic['invitegroup_ib'] != 3) | (df_nps_historic['invitegroup_ib'].isnull()))\n",
    "condition_3 = (df_nps_historic['invitegroup'] == 2)\n",
    "    \n",
    "df_nps_historic = df_nps_historic.loc[condition_1 & (condition_2 & condition_3)]\n",
    "\n",
    "# 2. Transform date column to datetime format\n",
    "delay_features = ['real_departure_time_local', 'scheduled_departure_time_local']\n",
    "for feat in delay_features:\n",
    "    df_nps_historic[feat] = pd.to_datetime(df_nps_historic[feat], format=\"%Y%m%d %H:%M:%S\", errors = 'coerce')\n",
    "            \n",
    "df_nps_historic['delay_departure'] = (df_nps_historic['real_departure_time_local'] - df_nps_historic['scheduled_departure_time_local']).dt.total_seconds()/60\n",
    "    \n",
    "# NPS\n",
    "df_nps_historic['date_flight_local'] = pd.to_datetime(df_nps_historic['date_flight_local'])\n",
    "\n",
    "# Load Factor\n",
    "df_lf_historic['flight_date_local'] = pd.to_datetime(df_lf_historic['flight_date_local'])\n",
    "\n",
    "# 3. Filter out covid years\n",
    "# NPS (historic)\n",
    "df_nps_historic = df_nps_historic[df_nps_historic['date_flight_local'].dt.year >= 2019]\n",
    "df_nps_historic = df_nps_historic[~df_nps_historic['date_flight_local'].dt.year.isin([2020, 2021])]\n",
    "    \n",
    "# Load factor (historic)\n",
    "df_lf_historic = df_lf_historic[df_lf_historic['flight_date_local'].dt.year >= 2019]\n",
    "df_lf_historic = df_lf_historic[~df_lf_historic['flight_date_local'].dt.year.isin([2020, 2021])]\n",
    "\n",
    "# 4. Create otp, promoter, detractor and load factor columns.\n",
    "# OTP\n",
    "df_nps_historic['otp15_takeoff'] = (df_nps_historic['delay'] > 15).astype(int)\n",
    "\n",
    "# Promoter and Detractor columns\n",
    "df_nps_historic[\"promoter_binary\"] = df_nps_historic[\"nps_category\"].apply(lambda x: 1 if x == \"Promoter\" else 0)\n",
    "df_nps_historic[\"detractor_binary\"] = df_nps_historic[\"nps_category\"].apply(lambda x: 1 if x == \"Detractor\" else 0)\n",
    "\n",
    "# Load Factor\n",
    "df_lf_historic['load_factor_business'] = df_lf_historic['pax_business'] / df_lf_historic['capacity_business']\n",
    "df_lf_historic['load_factor_premium_ec'] = df_lf_historic['pax_premium_ec'] / df_lf_historic['capacity_premium_ec']\n",
    "df_lf_historic['load_factor_economy'] = df_lf_historic['pax_economy'] / df_lf_historic['capacity_economy']\n",
    "\n",
    "\n",
    "# 5. Merge dataframes.\n",
    "cabin_to_load_factor_column = {\n",
    "    'Economy': 'load_factor_economy',\n",
    "    'Business': 'load_factor_business',\n",
    "    'Premium Economy': 'load_factor_premium_ec'\n",
    "}\n",
    "\n",
    "# HISTORIC\n",
    "if 'operating_carrier' in df_lf_historic.columns:\n",
    "    df_lf_historic.columns = ['date_flight_local' if x=='flight_date_local' else \n",
    "                                'operating_airline_code' if x=='operating_carrier' else\n",
    "                                'surveyed_flight_number' if x=='op_flight_num' else\n",
    "                                x for x in df_lf_historic.columns]\n",
    "elif 'op_carrier_group_ib' in df_lf_historic.columns:\n",
    "    df_lf_historic.columns = ['date_flight_local' if x=='flight_date_local' else \n",
    "                                'operating_airline_code' if x=='op_carrier_group_ib' else\n",
    "                                'surveyed_flight_number' if x=='op_flight_num' else\n",
    "                                x for x in df_lf_historic.columns]       \n",
    "    \n",
    "df_lf_historic['date_flight_local']=pd.to_datetime(df_lf_historic['date_flight_local'])\n",
    "df_lf_historic['surveyed_flight_number'] = df_lf_historic['surveyed_flight_number'].astype('float64')\n",
    "    \n",
    "# List of columns to transform\n",
    "load_factor_columns = ['load_factor_business', 'load_factor_premium_ec', 'load_factor_economy']\n",
    "\n",
    "# Automatically determine id_vars by excluding load_factor_columns from all columns\n",
    "id_vars = [col for col in df_lf_historic.columns if col not in load_factor_columns]\n",
    "\n",
    "# Reshaping the DataFrame while dynamically keeping all other columns\n",
    "df_lf_historic = pd.melt(df_lf_historic, id_vars=id_vars, \n",
    "                    value_vars=load_factor_columns,\n",
    "                    var_name='cabin_in_surveyed_flight', value_name='load_factor')\n",
    "\n",
    "# Replacing the column names in 'cabin_in_surveyed_flight' with the desired cabin types\n",
    "df_lf_historic['cabin_in_surveyed_flight'] = df_lf_historic['cabin_in_surveyed_flight'].map({\n",
    "    'load_factor_business': 'Business',\n",
    "    'load_factor_premium_ec': 'Premium Economy',\n",
    "    'load_factor_economy': 'Economy'\n",
    "})\n",
    "\n",
    "    \n",
    "df_historic = pd.merge(df_nps_historic, df_lf_historic, \n",
    "                    how='left', \n",
    "                    on=['date_flight_local', 'surveyed_flight_number', 'cabin_in_surveyed_flight', 'haul'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53b8fd-ff2a-4575-b3db-defd0404a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Condiciones de filtrado de fechas\n",
    "condition_start_o = (df_historic['date_flight_local'] >= STR_START_DATE_o)\n",
    "condition_end_o = (df_historic['date_flight_local'] <= STR_END_DATE_o)\n",
    "\n",
    "condition_start_f = (df_historic['date_flight_local'] >= STR_START_DATE_f)\n",
    "condition_end_f = (df_historic['date_flight_local'] <= STR_END_DATE_f)\n",
    "    \n",
    "# Filtrar cabinas; si STR_CABIN es \"All\", omitimos el filtro, de lo contrario usamos isin() para listas\n",
    "condition_cabin = df_historic['cabin_in_surveyed_flight'] == STR_CABIN\n",
    "\n",
    "# Filtrar haul; si STR_HAUL es \"All\", omitimos el filtro, de lo contrario usamos isin() para listas\n",
    "condition_haul = df_historic['haul'] == STR_HAUL\n",
    "\n",
    "# Filtrar el DataFrame utilizando todas las condiciones\n",
    "df_2023 = df_historic[condition_start_o & condition_end_o & condition_cabin & condition_haul]\n",
    "df_2024 = df_historic[condition_start_f & condition_end_f & condition_cabin & condition_haul]\n",
    "    \n",
    "n = len(df_2023)\n",
    "    \n",
    "df_historic['date_flight_local'] = pd.to_datetime(df_historic['date_flight_local'])    \n",
    "condition_month = (df_historic['date_flight_local'].dt.month == month)\n",
    "    \n",
    "df_sampled = df_historic[condition_cabin & condition_haul & condition_month].sample(n=n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f2e2a2-4d21-4f38-bf5e-3672e1079ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "415e4432-8ac4-467b-8e1f-5f9795126a74",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clusterize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e058240-0d82-4c7e-8f8e-4d8bad5c7665",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f035955-0fc1-420b-9663-289a1f90a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estudio del codo\n",
    "# inertia_values = elbow_study(df, variables, max_k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e50b1-be34-4a0c-80a3-bc9f6dfd5a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento con el número óptimo de clusters\n",
    "# df_clustered = train_kmeans(df, variables, n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c793b6a5-0eaa-4733-81c7-bce13416b884",
   "metadata": {},
   "source": [
    "## Inference with 3Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc0663-cb69-487a-bd4f-32cbe0164eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sampled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356577c5-ddb0-451d-9ead-a35fe8af5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = classify_into_clusters(df_sampled, touchpoints+labels, kmeans_scaler, kmeans_model)\n",
    "df_f = classify_into_clusters(df_2023, touchpoints+labels, kmeans_scaler, kmeans_model)\n",
    "df_r = classify_into_clusters(df_2024, touchpoints+labels, kmeans_scaler, kmeans_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e09dd-2bce-40f2-b738-7d501a154ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_variable_against_list(df, main_var, var_list, cluster_col='cluster'):\n",
    "    \"\"\"\n",
    "    Plots a main variable against each variable in a list with added noise for better cluster visualization.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the variables and cluster labels.\n",
    "    - main_var (str): Name of the main variable/column to plot on the x-axis.\n",
    "    - var_list (list of str): List of variable names to plot on the y-axis.\n",
    "    - cluster_col (str): Name of the column with cluster labels. Default is 'cluster'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add noise to the main variable\n",
    "    df[f'{main_var}_noisy'] = df[main_var] + np.random.uniform(-0.5, 0.5, df.shape[0])\n",
    "    \n",
    "    for var in var_list:\n",
    "        # Add noise to the current variable in var_list\n",
    "        df[f'{var}_noisy'] = df[var] + np.random.uniform(-0.5, 0.5, df.shape[0])\n",
    "        \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(df[f'{main_var}_noisy'], df[f'{var}_noisy'], c=df[cluster_col], cmap='viridis', alpha=0.7)\n",
    "        plt.colorbar(label='Cluster')\n",
    "        plt.xlabel(f'{main_var} (noisy)')\n",
    "        plt.ylabel(f'{var} (noisy)')\n",
    "        plt.title(f'{main_var} vs. {var} with Cluster Labels (with Noise)')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "main_var = 'pun_100_punctuality'\n",
    "\n",
    "# Create a copy of the list without the main_var element\n",
    "plot_variable_against_list(df_f, main_var, [var for var in touchpoints if var != main_var], cluster_col='cluster')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b378a9-c46c-482a-a7cc-aa38da5586c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_f_0 = df_f[df_f['cluster']==0]\n",
    "df_f_1 = df_f[df_f['cluster']==1]\n",
    "df_f_2 = df_f[df_f['cluster']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e21b7-07ea-43a8-a3fe-c82d814899b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "numeric_columns = [col for col in df.columns if col.endswith('_nps')]\n",
    "\n",
    "cluster_df = df_f_0.copy()\n",
    "\n",
    "# Ajustar estilo de los gráficos (opcional)\n",
    "plt.style.use('ggplot')  # Puedes elegir otro estilo disponible\n",
    "\n",
    "# Generar histogramas y boxplots\n",
    "for col in numeric_columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Histograma\n",
    "    axes[0].hist(cluster_df[col].dropna(), bins=50, edgecolor='k', alpha=0.7)\n",
    "    axes[0].set_title(f\"Histograma de {col}\")\n",
    "    axes[0].set_xlabel(col)\n",
    "    axes[0].set_ylabel(\"Frecuencia\")\n",
    "\n",
    "    # Boxplot\n",
    "    axes[1].boxplot(cluster_df[col].dropna(), vert=False)\n",
    "    axes[1].set_title(f\"Boxplot de {col}\")\n",
    "    axes[1].set_xlabel(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188dbbc7-a975-4fc9-9ad6-1f4964e4866b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Predict explain original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74902aef-c8bb-47b3-8e5b-a2e0d5b1938f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = op_vars + touchpoints\n",
    "columns = [\n",
    "    \"respondent_id\",\n",
    "    'cabin_in_surveyed_flight', \n",
    "    'haul',\n",
    "    \"date_flight_local\",\n",
    "    \"otp15_takeoff\",\n",
    "    \"ticket_price\",\n",
    "    \"load_factor\",\n",
    "    \"pun_100_punctuality\",\n",
    "    \"bkg_200_journey_preparation\",\n",
    "    \"pfl_100_checkin\",\n",
    "    \"pfl_200_security\",\n",
    "    \"pfl_300_lounge\",\n",
    "    \"pfl_500_boarding\",\n",
    "    \"ifl_300_cabin\",\n",
    "    \"ifl_200_flight_crew_annoucements\",\n",
    "    \"ifl_600_wifi\",\n",
    "    \"ifl_500_ife\",\n",
    "    \"ifl_400_food_drink\",\n",
    "    \"ifl_100_cabin_crew\",\n",
    "    \"arr_100_arrivals\",\n",
    "    \"con_100_connections\",\n",
    "    \"loy_200_loyalty_programme\",\n",
    "    \"img_310_ease_contact_phone\",\n",
    "    \"promoter_binary\",\n",
    "    \"detractor_binary\",\n",
    "    \"monthly_weight\",\n",
    "    \"cluster\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1259c5-ab04-4ced-ac1d-efc2165e4d15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_f = df_f[columns]\n",
    "df_r = df_r[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5b95a-0de8-4a43-8a1c-a689bb21abb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_probabilities_2023 = predict_and_explain(clf_model[model_names[0]], clf_model[model_names[1]], df_f, features, K_uncertainty=5)\n",
    "df_probabilities_2024 = predict_and_explain(clf_model[model_names[0]], clf_model[model_names[1]], df_r, features, K_uncertainty=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea7dc0-4f0d-4e8f-9d93-f9a265be8014",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad5a69d-265f-4eb1-b43b-3fe997df2dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agg_df_2023 = calculate_metrics_summary(df_probabilities_2023, STR_START_DATE_o, STR_END_DATE_o, touchpoints)\n",
    "\n",
    "# Calcular métricas agregadas para el año siguiente\n",
    "agg_df_2024 = calculate_metrics_summary(df_probabilities_2024, STR_START_DATE_f, STR_END_DATE_f, touchpoints)\n",
    "\n",
    "HEADERS_TARGETS = [\n",
    "    \"ticket_price\",\n",
    "    \"load_factor\",\n",
    "    \"pun_100_punctuality_satisfaction\",\n",
    "    \"bkg_200_journey_preparation_satisfaction\",\n",
    "    \"pfl_100_checkin_satisfaction\",\n",
    "    \"pfl_200_security_satisfaction\",\n",
    "    \"pfl_300_lounge_satisfaction\",\n",
    "    \"pfl_500_boarding_satisfaction\",\n",
    "    \"ifl_300_cabin_satisfaction\",\n",
    "    \"ifl_200_flight_crew_annoucements_satisfaction\",\n",
    "    \"ifl_600_wifi_satisfaction\",\n",
    "    \"ifl_500_ife_satisfaction\",\n",
    "    \"ifl_400_food_drink_satisfaction\",\n",
    "    \"ifl_100_cabin_crew_satisfaction\",\n",
    "    \"arr_100_arrivals_satisfaction\",\n",
    "    \"con_100_connections_satisfaction\",\n",
    "    \"loy_200_loyalty_programme_satisfaction\",\n",
    "    \"img_310_ease_contact_phone_satisfaction\"\n",
    "]\n",
    "\n",
    "\n",
    "targets = agg_df_2024[HEADERS_TARGETS]\n",
    "            \n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7477f642-a1e0-4766-973b-7cba5fd833dd",
   "metadata": {},
   "source": [
    "### Naive client based optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe5d2c-1864-4eb1-8912-0a3e6c2b34c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def causal_swapping(df, targets, touchpoints, df_original, max_iterations=100000, threshold=0.05, patience=500, k=5):\n",
    "    \"\"\"\n",
    "    Adjusts the customer population by swapping clients between clusters until targets are met or no further improvement.\n",
    "    Ensures final satisfactions are within desired intervals based on the necessary change.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with customer scores.\n",
    "        targets (pd.DataFrame): DataFrame with satisfaction targets.\n",
    "        touchpoints (list): List of touchpoints to adjust.\n",
    "        df_original (pd.DataFrame): Original DataFrame before any adjustments.\n",
    "        max_iterations (int): Maximum number of iterations.\n",
    "        threshold (float): Threshold to consider when adjustments have reached the target.\n",
    "        patience (int): Number of additional iterations after the distance stops improving.\n",
    "        k (int): Number of top touchpoints to consider when determining direction.\n",
    "    Returns:\n",
    "        pd.DataFrame: Adjusted DataFrame.\n",
    "    \"\"\"\n",
    "    df_adjusted = df.copy()\n",
    "\n",
    "    # Calculate original satisfactions and initial values\n",
    "    original_satisfactions = {tp: calculate_satisfaction(df_original, tp) for tp in touchpoints}\n",
    "    initial_distance = calculate_total_distance(df_adjusted, targets, touchpoints)\n",
    "    iteration = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        adjustments_made = False\n",
    "        local_adjustments_made = False\n",
    "\n",
    "        # Determine direction of adjustment based on the top k touchpoints\n",
    "        direction = 0  # Positive for increase, negative for decrease\n",
    "        total_increase_needed = 0\n",
    "        total_decrease_needed = 0\n",
    "\n",
    "        for tp in touchpoints[:k]:\n",
    "            current_satisfaction = calculate_satisfaction(df_adjusted, tp)\n",
    "            target_value = targets[f'{tp}_satisfaction'].values[0]\n",
    "            original_satisfaction = original_satisfactions[tp]\n",
    "\n",
    "            # Determine desired interval\n",
    "            if original_satisfaction < target_value:\n",
    "                lower_bound, upper_bound = target_value, target_value + threshold\n",
    "            elif original_satisfaction > target_value:\n",
    "                lower_bound, upper_bound = target_value - threshold, target_value\n",
    "            else:\n",
    "                lower_bound, upper_bound = original_satisfaction - threshold, original_satisfaction + threshold\n",
    "\n",
    "            # Calculate direction needed for the current touchpoint\n",
    "            if current_satisfaction < lower_bound:\n",
    "                increase_needed = lower_bound - current_satisfaction\n",
    "                total_increase_needed += increase_needed\n",
    "                direction += 1\n",
    "            elif current_satisfaction > upper_bound:\n",
    "                decrease_needed = current_satisfaction - upper_bound\n",
    "                total_decrease_needed += decrease_needed\n",
    "                direction -= 1\n",
    "\n",
    "        # Determine overall direction based on the sum of needs\n",
    "        if total_increase_needed > total_decrease_needed:\n",
    "            increase_satisfaction = True\n",
    "        else:\n",
    "            increase_satisfaction = False\n",
    "\n",
    "        # Apply swap based on the direction determined\n",
    "        if increase_satisfaction:\n",
    "            # Increase satisfaction: swap from cluster 2 to cluster 0\n",
    "            df_adjusted, local_adjustments_made = swap_client_based_on_out_prob(df_adjusted, from_cluster=2, to_cluster=0, increase=True)\n",
    "        else:\n",
    "            # Decrease satisfaction: swap from cluster 0 to cluster 2\n",
    "            df_adjusted, local_adjustments_made = swap_client_based_on_out_prob(df_adjusted, from_cluster=0, to_cluster=2, increase=False)\n",
    "\n",
    "        # Check if adjustment was made and recalculate distance\n",
    "        if local_adjustments_made:\n",
    "            new_distance = calculate_total_distance(df_adjusted, targets, touchpoints)\n",
    "\n",
    "            # If new distance is better, reset patience counter\n",
    "            if new_distance < initial_distance:\n",
    "                initial_distance = new_distance\n",
    "                patience_counter = 0\n",
    "                adjustments_made = True\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Local minimum reached at iteration {iteration} with distance {initial_distance:.4f} and patience count {patience_counter}\")\n",
    "            break\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return df_adjusted\n",
    "\n",
    "def swap_client_based_on_out_prob(df_adjusted, from_cluster, to_cluster, increase=True):\n",
    "    \"\"\"\n",
    "    Helper function to swap clients between clusters based on out_prob_nps values.\n",
    "    Args:\n",
    "        df_adjusted (pd.DataFrame): Adjusted DataFrame.\n",
    "        from_cluster (int): Cluster to remove clients from.\n",
    "        to_cluster (int): Cluster to add clients to.\n",
    "        increase (bool): Whether we are increasing or decreasing satisfaction.\n",
    "    Returns:\n",
    "        (pd.DataFrame, bool): Tuple containing the adjusted DataFrame and a boolean indicating if the swap was successful.\n",
    "    \"\"\"\n",
    "    clients_from = df_adjusted[df_adjusted['cluster'] == from_cluster]\n",
    "    clients_to = df_adjusted[df_adjusted['cluster'] == to_cluster]\n",
    "\n",
    "    if not clients_from.empty and not clients_to.empty:\n",
    "        if increase:\n",
    "            # Select client with high out_prob_nps from 'to_cluster'\n",
    "            client_to_add = clients_to[clients_to['out_prob_nps'] > clients_to['out_prob_nps'].quantile(0.90)].sample(n=1)\n",
    "            if client_to_add.empty:\n",
    "                return df_adjusted, False\n",
    "\n",
    "            # Select client with low out_prob_nps from 'from_cluster'\n",
    "            client_to_remove = clients_from[clients_from['out_prob_nps'] < clients_from['out_prob_nps'].quantile(0.10)].sample(n=1)\n",
    "            if client_to_remove.empty:\n",
    "                return df_adjusted, False\n",
    "        else:\n",
    "            # Select client with low out_prob_nps from 'to_cluster'\n",
    "            client_to_add = clients_to[clients_to['out_prob_nps'] < clients_to['out_prob_nps'].quantile(0.10)].sample(n=1)\n",
    "            if client_to_add.empty:\n",
    "                return df_adjusted, False\n",
    "\n",
    "            # Select client with high out_prob_nps from 'from_cluster'\n",
    "            client_to_remove = clients_from[clients_from['out_prob_nps'] > clients_from['out_prob_nps'].quantile(0.90)].sample(n=1)\n",
    "            if client_to_remove.empty:\n",
    "                return df_adjusted, False\n",
    "\n",
    "        # Update df_adjusted by adding and removing clients\n",
    "        df_adjusted = pd.concat([df_adjusted, client_to_add], ignore_index=True)\n",
    "        df_adjusted.drop(client_to_remove.index, inplace=True)\n",
    "\n",
    "        return df_adjusted.copy(), True\n",
    "\n",
    "    return df_adjusted, False\n",
    "\n",
    "def simple_swap_clients_between_clusters(df_adjusted, from_cluster, to_cluster):\n",
    "    \"\"\"\n",
    "    Helper function to simply swap clients between clusters.\n",
    "    Args:\n",
    "        df_adjusted (pd.DataFrame): Adjusted DataFrame.\n",
    "        from_cluster (int): Cluster to remove clients from.\n",
    "        to_cluster (int): Cluster to add clients to.\n",
    "    Returns:\n",
    "        bool: True if swap was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    clients_from = df_adjusted[df_adjusted['cluster'] == from_cluster]\n",
    "    clients_to = df_adjusted[df_adjusted['cluster'] == to_cluster]\n",
    "\n",
    "    if not clients_from.empty and not clients_to.empty:\n",
    "        # Select a random client from 'from_cluster'\n",
    "        client_to_remove = clients_from.sample(n=1)\n",
    "        df_adjusted.drop(client_to_remove.index, inplace=True)\n",
    "\n",
    "        # Select a random client from 'to_cluster'\n",
    "        client_to_add = clients_to.sample(n=1)\n",
    "        df_adjusted = pd.concat([df_adjusted, client_to_add], ignore_index=True)\n",
    "\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def within_interval(current, original, target, threshold, sum_differences):\n",
    "    \"\"\"Helper function to determine if satisfaction is within the desired interval.\"\"\"\n",
    "    if original < target:\n",
    "        return target <= current <= target + threshold\n",
    "    elif original > target:\n",
    "        return target - threshold <= current <= target\n",
    "    else:\n",
    "        if sum_differences > 0:\n",
    "            return original <= current <= original + threshold\n",
    "        else:\n",
    "            return original - threshold <= current <= original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26db6e8-04c0-49e2-93d4-a0e28f52b7ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4e437-6a79-480c-a6bf-8a32b3a25571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def causal_swapping(df, targets, touchpoints, df_original, max_iterations=100000, threshold=0.05, patience=500, k=10):\n",
    "    \"\"\"\n",
    "    Adjusts the customer population by swapping clients between clusters until targets are met or no further improvement.\n",
    "    Ensures final satisfactions are within desired intervals based on the necessary change.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with customer scores.\n",
    "        targets (pd.DataFrame): DataFrame with satisfaction targets.\n",
    "        touchpoints (list): List of touchpoints to adjust.\n",
    "        df_original (pd.DataFrame): Original DataFrame before any adjustments.\n",
    "        max_iterations (int): Maximum number of iterations.\n",
    "        threshold (float): Threshold to consider when adjustments have reached the target.\n",
    "        patience (int): Number of additional iterations after the distance stops improving.\n",
    "        k (int): Number of top touchpoints to consider when determining direction.\n",
    "    Returns:\n",
    "        pd.DataFrame: Adjusted DataFrame.\n",
    "    \"\"\"\n",
    "    df_adjusted = df.copy()\n",
    "\n",
    "    # Calculate original satisfactions and initial values\n",
    "    original_satisfactions = {tp: calculate_satisfaction(df_original, tp) for tp in touchpoints}\n",
    "    initial_distance = calculate_total_distance(df_adjusted, targets, touchpoints)\n",
    "    iteration = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        adjustments_made = False\n",
    "        local_adjustments_made = False\n",
    "\n",
    "        # Determine direction of adjustment based on the top k touchpoints\n",
    "        direction = 0  # Positive for increase, negative for decrease\n",
    "        total_increase_needed = 0\n",
    "        total_decrease_needed = 0\n",
    "        selected_touchpoint = None\n",
    "\n",
    "        for tp in touchpoints[:k]:\n",
    "            current_satisfaction = calculate_satisfaction(df_adjusted, tp)\n",
    "            target_value = targets[f'{tp}_satisfaction'].values[0]\n",
    "            original_satisfaction = original_satisfactions[tp]\n",
    "\n",
    "            # Determine desired interval\n",
    "            if original_satisfaction < target_value:\n",
    "                lower_bound, upper_bound = target_value, target_value + threshold\n",
    "            elif original_satisfaction > target_value:\n",
    "                lower_bound, upper_bound = target_value - threshold, target_value\n",
    "            else:\n",
    "                lower_bound, upper_bound = original_satisfaction - threshold, original_satisfaction + threshold\n",
    "\n",
    "            # Calculate direction needed for the current touchpoint\n",
    "            if current_satisfaction < lower_bound:\n",
    "                increase_needed = lower_bound - current_satisfaction\n",
    "                total_increase_needed += increase_needed\n",
    "                direction += 1\n",
    "                # Select the first touchpoint with the same nature of adjustment\n",
    "                if selected_touchpoint is None:\n",
    "                    selected_touchpoint = tp\n",
    "            elif current_satisfaction > upper_bound:\n",
    "                decrease_needed = current_satisfaction - upper_bound\n",
    "                total_decrease_needed += decrease_needed\n",
    "                direction -= 1\n",
    "                # Select the first touchpoint with the same nature of adjustment\n",
    "                if selected_touchpoint is None:\n",
    "                    selected_touchpoint = tp\n",
    "\n",
    "        # Determine overall direction based on the sum of needs\n",
    "        if total_increase_needed > total_decrease_needed:\n",
    "            increase_satisfaction = True\n",
    "        else:\n",
    "            increase_satisfaction = False\n",
    "\n",
    "        # Apply swap based on the direction determined, using only the selected touchpoint\n",
    "        if increase_satisfaction:\n",
    "            # Increase satisfaction: swap from cluster 2 to cluster 0\n",
    "            df_adjusted, local_adjustments_made = swap_client_based_on_out_prob(df_adjusted, from_cluster=0, to_cluster=2, increase=True, touchpoints=[selected_touchpoint])\n",
    "        else:\n",
    "            # Decrease satisfaction: swap from cluster 0 to cluster 2\n",
    "            df_adjusted, local_adjustments_made = swap_client_based_on_out_prob(df_adjusted, from_cluster=2, to_cluster=0, increase=False, touchpoints=[selected_touchpoint])\n",
    "\n",
    "        # Check if adjustment was made and recalculate distance\n",
    "        if local_adjustments_made:\n",
    "            new_distance = calculate_total_distance(df_adjusted, targets, touchpoints)\n",
    "\n",
    "            # If new distance is better, reset patience counter\n",
    "            if new_distance < initial_distance:\n",
    "                initial_distance = new_distance\n",
    "                patience_counter = 0\n",
    "                adjustments_made = True\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Local minimum reached at iteration {iteration} with distance {initial_distance:.4f} and patience count {patience_counter}\")\n",
    "            break\n",
    "            \n",
    "        print(f'Iteration {iteration} with distance {new_distance:.4f}')\n",
    "\n",
    "        iteration += 1\n",
    "        \n",
    "\n",
    "    return df_adjusted\n",
    "\n",
    "def swap_client_based_on_out_prob(df_adjusted, from_cluster, to_cluster, increase=True, touchpoints=None):\n",
    "    \"\"\"\n",
    "    Helper function to swap clients between clusters based on out_prob_nps values and conditions on touchpoints.\n",
    "    Args:\n",
    "        df_adjusted (pd.DataFrame): Adjusted DataFrame.\n",
    "        from_cluster (int): Cluster to remove clients from.\n",
    "        to_cluster (int): Cluster to add clients to.\n",
    "        increase (bool): Whether we are increasing or decreasing satisfaction.\n",
    "        touchpoints (list): List of touchpoints to consider for criteria.\n",
    "    Returns:\n",
    "        (pd.DataFrame, bool): Tuple containing the adjusted DataFrame and a boolean indicating if the swap was successful.\n",
    "    \"\"\"\n",
    "    clients_from = df_adjusted[df_adjusted['cluster'] == from_cluster]\n",
    "    clients_to = df_adjusted[df_adjusted['cluster'] == to_cluster]\n",
    "\n",
    "    if not clients_from.empty and not clients_to.empty:\n",
    "        if increase:\n",
    "            # Iterate over clients in 'to_cluster' with high out_prob_nps until conditions are met\n",
    "            # clients_to_candidates = clients_to[clients_to['out_prob_nps'] > clients_to['out_prob_nps'].quantile(0.90)]\n",
    "            clients_to_candidates = clients_to[clients_to['out_prob_nps'] > 0]\n",
    "            for _, client_to_add in clients_to_candidates.iterrows():\n",
    "                # clients_from_candidates = clients_from[clients_from['out_prob_nps'] < clients_from['out_prob_nps'].quantile(0.10)]\n",
    "                clients_from_candidates = clients_from[clients_from['out_prob_nps'] < 0]\n",
    "                for _, client_to_remove in clients_from_candidates.iterrows():\n",
    "                    # Check conditions for touchpoints, only if the satisfaction for that touchpoint needed an increase\n",
    "                    valid_swap = True\n",
    "                    if touchpoints:\n",
    "                        for tp in touchpoints:\n",
    "                            if increase and (client_to_add[f'{tp}_nps'] <= client_to_remove[f'{tp}_nps'] or client_to_add[tp] <= client_to_remove[tp]):\n",
    "                                valid_swap = False\n",
    "                                break\n",
    "                            elif not increase and (client_to_add[f'{tp}_nps'] >= client_to_remove[f'{tp}_nps'] or client_to_add[tp] >= client_to_remove[tp]):\n",
    "                                valid_swap = False\n",
    "                                break\n",
    "                        if valid_swap:\n",
    "                            # Perform swap\n",
    "                            df_adjusted = pd.concat([df_adjusted, client_to_add.to_frame().T], ignore_index=True)\n",
    "                            df_adjusted.drop(client_to_remove.name, inplace=True)\n",
    "                            return df_adjusted.copy(), True\n",
    "\n",
    "            # Relax condition: ignore SHAP value comparison\n",
    "            for _, client_to_add in clients_to_candidates.iterrows():\n",
    "                # clients_from_candidates = clients_from[clients_from['out_prob_nps'] < clients_from['out_prob_nps'].quantile(0.10)]\n",
    "                clients_from_candidates = clients_from[clients_from['out_prob_nps'] < 0]\n",
    "                for _, client_to_remove in clients_from_candidates.iterrows():\n",
    "                    # Check condition only for touchpoint value\n",
    "                    if touchpoints:\n",
    "                        if increase:\n",
    "                            for tp in touchpoints:\n",
    "                                if client_to_add[tp] <= client_to_remove[tp]:\n",
    "                                    continue\n",
    "                        else:\n",
    "                            for tp in touchpoints:\n",
    "                                if client_to_add[tp] >= client_to_remove[tp]:\n",
    "                                    continue\n",
    "                    # Perform swap\n",
    "                    df_adjusted = pd.concat([df_adjusted, client_to_add.to_frame().T], ignore_index=True)\n",
    "                    df_adjusted.drop(client_to_remove.name, inplace=True)\n",
    "                    return df_adjusted.copy(), True\n",
    "\n",
    "            # Relax condition further: ignore touchpoint value comparison\n",
    "            if not clients_to_candidates.empty and not clients_from_candidates.empty:\n",
    "                client_to_add = clients_to_candidates.sample(n=1)\n",
    "                client_to_remove = clients_from_candidates.sample(n=1)\n",
    "                df_adjusted = pd.concat([df_adjusted, client_to_add], ignore_index=True)\n",
    "                df_adjusted.drop(client_to_remove.index, inplace=True)\n",
    "                return df_adjusted.copy(), True\n",
    "\n",
    "        else:\n",
    "            # Decrease satisfaction: Opposite logic\n",
    "            # clients_to_candidates = clients_to[clients_to['out_prob_nps'] < clients_to['out_prob_nps'].quantile(0.10)]\n",
    "            clients_to_candidates = clients_to[clients_to['out_prob_nps'] < 0]\n",
    "            for _, client_to_add in clients_to_candidates.iterrows():\n",
    "                # clients_from_candidates = clients_from[clients_from['out_prob_nps'] > clients_from['out_prob_nps'].quantile(0.90)]\n",
    "                clients_from_candidates = clients_from[clients_from['out_prob_nps'] > 0]\n",
    "                for _, client_to_remove in clients_from_candidates.iterrows():\n",
    "                    # Check conditions for touchpoints, only if the satisfaction for that touchpoint needed a decrease\n",
    "                    valid_swap = True\n",
    "                    if touchpoints:\n",
    "                        for tp in touchpoints:\n",
    "                            if increase and (client_to_add[f'{tp}_nps'] <= client_to_remove[f'{tp}_nps'] or client_to_add[tp] <= client_to_remove[tp]):\n",
    "                                valid_swap = False\n",
    "                                break\n",
    "                            elif not increase and (client_to_add[f'{tp}_nps'] >= client_to_remove[f'{tp}_nps'] or client_to_add[tp] >= client_to_remove[tp]):\n",
    "                                valid_swap = False\n",
    "                                break\n",
    "                        if valid_swap:\n",
    "                            # Perform swap\n",
    "                            df_adjusted = pd.concat([df_adjusted, client_to_add.to_frame().T], ignore_index=True)\n",
    "                            df_adjusted.drop(client_to_remove.name, inplace=True)\n",
    "                            return df_adjusted.copy(), True\n",
    "\n",
    "            # Relax condition: ignore SHAP value comparison\n",
    "            for _, client_to_add in clients_to_candidates.iterrows():\n",
    "                # clients_from_candidates = clients_from[clients_from['out_prob_nps'] > clients_from['out_prob_nps'].quantile(0.90)]\n",
    "                clients_from_candidates = clients_from[clients_from['out_prob_nps'] > 0]\n",
    "                for _, client_to_remove in clients_from_candidates.iterrows():\n",
    "                    # Check condition only for touchpoint value\n",
    "                    if touchpoints:\n",
    "                        if increase:\n",
    "                            for tp in touchpoints:\n",
    "                                if client_to_add[tp] <= client_to_remove[tp]:\n",
    "                                    continue\n",
    "                        else:\n",
    "                            for tp in touchpoints:\n",
    "                                if client_to_add[tp] >= client_to_remove[tp]:\n",
    "                                    continue\n",
    "                    # Perform swap\n",
    "                    df_adjusted = pd.concat([df_adjusted, client_to_add.to_frame().T], ignore_index=True)\n",
    "                    df_adjusted.drop(client_to_remove.name, inplace=True)\n",
    "                    return df_adjusted.copy(), True\n",
    "\n",
    "            # Relax condition further: ignore touchpoint value comparison\n",
    "            if not clients_to_candidates.empty and not clients_from_candidates.empty:\n",
    "                client_to_add = clients_to_candidates.sample(n=1)\n",
    "                client_to_remove = clients_from_candidates.sample(n=1)\n",
    "                df_adjusted = pd.concat([df_adjusted, client_to_add], ignore_index=True)\n",
    "                df_adjusted.drop(client_to_remove.index, inplace=True)\n",
    "                return df_adjusted.copy(), True\n",
    "\n",
    "    return df_adjusted, False\n",
    "\n",
    "def simple_swap_clients_between_clusters(df_adjusted, from_cluster, to_cluster):\n",
    "    \"\"\"\n",
    "    Helper function to simply swap clients between clusters.\n",
    "    Args:\n",
    "        df_adjusted (pd.DataFrame): Adjusted DataFrame.\n",
    "        from_cluster (int): Cluster to remove clients from.\n",
    "        to_cluster (int): Cluster to add clients to.\n",
    "    Returns:\n",
    "        bool: True if swap was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    clients_from = df_adjusted[df_adjusted['cluster'] == from_cluster]\n",
    "    clients_to = df_adjusted[df_adjusted['cluster'] == to_cluster]\n",
    "\n",
    "    if not clients_from.empty and not clients_to.empty:\n",
    "        # Select a random client from 'from_cluster'\n",
    "        client_to_remove = clients_from.sample(n=1)\n",
    "        df_adjusted.drop(client_to_remove.index, inplace=True)\n",
    "\n",
    "        # Select a random client from 'to_cluster'\n",
    "        client_to_add = clients_to.sample(n=1)\n",
    "        df_adjusted = pd.concat([df_adjusted, client_to_add], ignore_index=True)\n",
    "\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def within_interval(current, original, target, threshold, sum_differences):\n",
    "    \"\"\"Helper function to determine if satisfaction is within the desired interval.\"\"\"\n",
    "    if original < target:\n",
    "        return target <= current <= target + threshold\n",
    "    elif original > target:\n",
    "        return target - threshold <= current <= target\n",
    "    else:\n",
    "        if sum_differences > 0:\n",
    "            return original <= current <= original + threshold\n",
    "        else:\n",
    "            return original - threshold <= current <= original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03413b4-f32f-4a81-ad65-d277036ed182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_distance(df, targets, touchpoints):\n",
    "    \"\"\"\n",
    "    Calcula la distancia total entre las satisfacciones actuales y los targets utilizando pesos suaves.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame con las puntuaciones de los clientes.\n",
    "    targets (pd.DataFrame): DataFrame con una fila que contiene los targets de satisfacción.\n",
    "    touchpoints (list): Lista de touchpoints a considerar.\n",
    "\n",
    "    Returns:\n",
    "    float: Distancia total ponderada.\n",
    "    \"\"\"\n",
    "    current_satisfaction = np.array([calculate_satisfaction(df, tp) for tp in touchpoints])\n",
    "    target_values = targets[[f'{tp}_satisfaction' for tp in touchpoints]].values.flatten()\n",
    "\n",
    "    ## get the euclidean distance between the two arrays\n",
    "    total_distance = np.linalg.norm(current_satisfaction - target_values)\n",
    "\n",
    "    return total_distance\n",
    "\n",
    "def causal_swapping(df, targets, touchpoints, max_iterations=100000, threshold=1, patience=5000):\n",
    "    \"\"\"\n",
    "    Adjusts the customer population by swapping clients between clusters until targets are met or no further improvement.\n",
    "    Ensures final satisfactions are within desired intervals based on the necessary change.\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with customer scores.\n",
    "        targets (pd.DataFrame): DataFrame with satisfaction targets.\n",
    "        touchpoints (list): List of touchpoints to adjust.\n",
    "        df_original (pd.DataFrame): Original DataFrame before any adjustments.\n",
    "        max_iterations (int): Maximum number of iterations.\n",
    "        threshold (float): Threshold to consider when adjustments have reached the target.\n",
    "        patience (int): Number of additional iterations after the distance stops improving.\n",
    "    Returns:\n",
    "        pd.DataFrame: Adjusted DataFrame.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    patience_counter = 0\n",
    "    df_original = df.copy()\n",
    "    df_current = df.copy()\n",
    "\n",
    "    original_satisfactions = {tp: calculate_satisfaction(df_original, tp) for tp in touchpoints}\n",
    "    initial_distance = calculate_total_distance(df_original, targets, touchpoints)\n",
    "    print(\"Initial distance between dataframe satisfactions and targets:\", initial_distance)\n",
    "\n",
    "    ## display initial satisfactions in a dataframe\n",
    "    initial_conditions_df = pd.DataFrame({\n",
    "        'Original Satisfaction': [original_satisfactions[tp] for tp in touchpoints],\n",
    "        'Lower Bound': [targets[f'{tp}_satisfaction'].values[0] - threshold for tp in touchpoints],\n",
    "        'Target': [targets[f'{tp}_satisfaction'].values[0] for tp in touchpoints],\n",
    "        'Upper Bound': [targets[f'{tp}_satisfaction'].values[0] + threshold for tp in touchpoints],\n",
    "    }, index=touchpoints).T\n",
    "\n",
    "    display(initial_conditions_df)\n",
    "\n",
    "    target_intervals = initial_conditions_df.loc[['Lower Bound', 'Upper Bound']].to_dict('list')\n",
    "    target_values = targets[[f'{tp}_satisfaction' for tp in touchpoints]].values.flatten()\n",
    "    current_satisfactions = np.array([calculate_satisfaction(df_current, tp) for tp in touchpoints])\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "\n",
    "\n",
    "        adjustments_made = False\n",
    "        mean_difference = np.mean(current_satisfactions - target_values)\n",
    "\n",
    "        if iteration % 250 == 0:\n",
    "            print(\"Iteration number: \", iteration)\n",
    "            print(\"Mean difference: \", mean_difference)\n",
    "\n",
    "        df_test = df_current.copy()\n",
    "\n",
    "        if mean_difference < 0:\n",
    "            ## if the mean difference is negative, increase satisfaction (swap a client from level 0 to level 2)\n",
    "            adjustments_made, df_test = swap_client_with_criteria(df_test, from_cluster=0, to_cluster=2, touchpoints=touchpoints)\n",
    "        elif mean_difference > 0:\n",
    "            ## if the mean difference is positive, decrease satisfaction (swap a client from level 2 to level 0)\n",
    "            adjustments_made, df_test = swap_client_with_criteria(df_test, from_cluster=2, to_cluster=0, touchpoints=touchpoints)\n",
    "\n",
    "        ## if an adjustment was made, recalculate the distance\n",
    "        if adjustments_made:\n",
    "\n",
    "            new_distance = calculate_total_distance(df_test, targets, touchpoints)\n",
    "\n",
    "            if iteration % 250 == 0 : # \n",
    "                print(f\"After adjustment, distance: {new_distance}\")\n",
    "\n",
    "\n",
    "            if new_distance < initial_distance:\n",
    "                ## if the distance is smaller, we include the adjustment to 'df_current' and recalculate satisfactions                \n",
    "                df_current = df_test.copy()\n",
    "                current_satisfactions = np.array([calculate_satisfaction(df_current, tp) for tp in touchpoints])\n",
    "\n",
    "                initial_distance = new_distance\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if iteration % 250 == 0: # \n",
    "                    print(f\"Unnefficient iteration ({iteration}). Patience counter is: {patience_counter}\")\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Local minimum reached at iteration {iteration} with distance {initial_distance:.4f}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"No more adjustments can be made at iteration {iteration}.\")\n",
    "            break\n",
    "\n",
    "        if new_distance < 1:\n",
    "            print(\"New_distance < 1. Stopping.\")\n",
    "            break        \n",
    "        elif all_within_limits(current_satisfactions, target_intervals, touchpoints):\n",
    "            print(f\"All satisfactions are within the target intervals at iteration {iteration}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    ## display final satisfactions in a dataframe\n",
    "    final_satisfactions = {tp: calculate_satisfaction(df_current, tp) for tp in touchpoints}\n",
    "    final_conditions_df = pd.DataFrame({\n",
    "        'Final Satisfactions': [final_satisfactions[tp] for tp in touchpoints],\n",
    "        'Lower Bound': [targets[f'{tp}_satisfaction'].values[0] - threshold for tp in touchpoints],\n",
    "        'Target': [targets[f'{tp}_satisfaction'].values[0] for tp in touchpoints],\n",
    "        'Upper Bound': [targets[f'{tp}_satisfaction'].values[0] + threshold for tp in touchpoints],\n",
    "    }, index=touchpoints).T\n",
    "    display(final_conditions_df)\n",
    "\n",
    "\n",
    "    return df_current\n",
    "\n",
    "\n",
    "def swap_client_with_criteria(df_adjusted, from_cluster, to_cluster, touchpoints, k=None, increase=True):\n",
    "    \"\"\"\n",
    "    Función auxiliar para intercambiar clientes entre clusters según criterios específicos.\n",
    "    Args:\n",
    "        df_adjusted (pd.DataFrame): DataFrame ajustado.\n",
    "        from_cluster (int): Cluster desde el que se eliminarán clientes.\n",
    "        to_cluster (int): Cluster al que se agregarán clientes.\n",
    "        touchpoints (list): Lista de touchpoints a considerar.\n",
    "        k (int): Número de variables principales a considerar para filtrar.\n",
    "        increase (bool): Indica si se debe aumentar o disminuir la satisfacción.\n",
    "    Returns:\n",
    "        bool: True si el intercambio fue exitoso, False en caso contrario.\n",
    "    \"\"\"\n",
    "    clients_from = df_adjusted[df_adjusted['cluster'] == from_cluster]\n",
    "    clients_to = df_adjusted[df_adjusted['cluster'] == to_cluster]\n",
    "\n",
    "    ## select k most important clusters\n",
    "    if k:\n",
    "        important_touchpoints = touchpoints[:k]\n",
    "    else:\n",
    "        important_touchpoints = touchpoints\n",
    "\n",
    "    if not clients_from.empty and not clients_to.empty:\n",
    "\n",
    "        ## select a client from the 'clients_from' dataframe to be removed\n",
    "        client_to_remove = clients_from.sample(n=1)\n",
    "        df_adjusted.drop(client_to_remove.index, inplace=True)\n",
    "\n",
    "        ## select a client from the 'clients_to' dataframe to be duplicated\n",
    "        client_to_add = clients_to.sample(n=1)\n",
    "        df_adjusted = pd.concat([df_adjusted, client_to_add], ignore_index=True) ## no se estan añadiendo bien los clientes\n",
    "        return True, df_adjusted\n",
    "    return False, df_adjusted\n",
    "\n",
    "def all_within_limits(satisfactions, intervals, touchpoints):\n",
    "    return all(\n",
    "        intervals[tp][0] <= satisfactions[i] <= intervals[tp][1]\n",
    "        for i, tp in enumerate(touchpoints)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4fe2bb-0f42-4045-a0c8-e38bb95086b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the causal adjust function\n",
    "# swaped_df = causal_swapping(df_probabilities_2023, targets, touchpoints, df_probabilities_2023)\n",
    "swaped_df = causal_swapping(df_probabilities_2023, targets, touchpoints, threshold=0.25)\n",
    "\n",
    "# # Apply the causal soft and hard simulations\n",
    "soft_sim_df = soft_manual_sim_causal(df_probabilities_2023, targets, touchpoints, df_probabilities_2023)\n",
    "# hard_sim_df_aux = hard_manual_sim_rand_cluster_causal(swaped_df, targets, touchpoints, df_f)\n",
    "# hard_sim_df = soft_manual_sim_causal(hard_sim_df_aux, targets, touchpoints, df_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ee76d-909f-4066-9e13-965bf9bb78d5",
   "metadata": {},
   "source": [
    "### Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca399e-ced6-4626-b3dd-0c107ed5a090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, beta\n",
    "\n",
    "def simulate_client_satisfaction_with_beta(\n",
    "    historical_df, \n",
    "    target_df, \n",
    "    num_simulations=1000, \n",
    "    beta_params=(2, 5)  # Parámetros alpha y beta, ajustables\n",
    "):\n",
    "    \"\"\"\n",
    "    Simula la satisfacción del cliente con distribuciones beta para touchpoints y normales para ticket_price y load_factor.\n",
    "    \n",
    "    Parameters:\n",
    "    - historical_df (DataFrame): DataFrame con respuestas históricas de clientes.\n",
    "    - target_df (DataFrame): DataFrame de una fila con los objetivos de satisfacción en cada touchpoint y valores medios\n",
    "      de 'ticket_price' y 'load_factor'.\n",
    "    - num_simulations (int): Número de simulaciones a realizar.\n",
    "    - beta_params (tuple): Parámetros alpha y beta para las distribuciones beta de los touchpoints.\n",
    "\n",
    "    Returns:\n",
    "    - simulated_df (DataFrame): DataFrame con las simulaciones generadas.\n",
    "    \"\"\"\n",
    "    # Extraer los objetivos y variables a partir de target_df\n",
    "    target_values = target_df.iloc[0]\n",
    "    touchpoints = [col.replace(\"_satisfaction\", \"\") for col in target_df.columns if '_satisfaction' in col]\n",
    "    ticket_price_mean = target_values['ticket_price']\n",
    "    load_factor_mean = target_values['load_factor']\n",
    "    \n",
    "    # Filtrar los touchpoints en el df histórico\n",
    "    touchpoint_columns = [col for col in historical_df.columns if any(tp in col for tp in touchpoints)]\n",
    "    touchpoint_df = historical_df[touchpoint_columns]\n",
    "    \n",
    "    # Calcular medias y correlaciones de los touchpoints históricos\n",
    "    touchpoint_means = touchpoint_df.mean()\n",
    "    correlation_matrix = touchpoint_df.corr()\n",
    "    \n",
    "    # Obtener la desviación estándar de las variables \"ticket_price\" y \"load_factor\"\n",
    "    ticket_price_std = historical_df['ticket_price'].std()\n",
    "    load_factor_std = historical_df['load_factor'].std()\n",
    "\n",
    "    # Generar simulaciones de satisfacción para los touchpoints en una distribución normal multivariante\n",
    "    simulated_touchpoints = np.random.multivariate_normal(\n",
    "        mean=touchpoint_means,\n",
    "        cov=correlation_matrix,\n",
    "        size=num_simulations\n",
    "    )\n",
    "\n",
    "    # Paso 1: Convertir las simulaciones de normal a uniformes (CDF de la normal)\n",
    "    uniform_touchpoints = norm.cdf(simulated_touchpoints)\n",
    "\n",
    "    # Paso 2: Convertir uniformes a beta, escalado al rango 0-10\n",
    "    simulated_touchpoints_beta = beta.ppf(uniform_touchpoints, *beta_params) * 10\n",
    "\n",
    "    # Generar simulaciones para 'ticket_price' y 'load_factor' con distribución normal\n",
    "    simulated_ticket_price = np.random.normal(ticket_price_mean, ticket_price_std, num_simulations)\n",
    "    simulated_load_factor = np.random.normal(load_factor_mean, load_factor_std, num_simulations)\n",
    "    \n",
    "    # Convertir las simulaciones en un DataFrame\n",
    "    simulated_df = pd.DataFrame(simulated_touchpoints_beta, columns=touchpoint_columns)\n",
    "    simulated_df['ticket_price'] = simulated_ticket_price\n",
    "    simulated_df['load_factor'] = simulated_load_factor\n",
    "\n",
    "    # Evaluar la similitud con los objetivos\n",
    "    for tp_col, tp_target in zip(touchpoint_columns, [target_values[f\"{tp}_satisfaction\"] for tp in touchpoints]):\n",
    "        # Verificar que >= 8 refleja el porcentaje deseado en targets (asumidos en escala 0-100)\n",
    "        satisfaction_percentage = (simulated_df[tp_col] >= 8).mean() * 100\n",
    "        print(f\"{tp_col} - Objetivo: {tp_target}%, Simulado: {satisfaction_percentage:.2f}%\")\n",
    "\n",
    "    print(f\"ticket_price - Objetivo: {target_values['ticket_price']}, Simulado: {simulated_df['ticket_price'].mean():.2f}\")\n",
    "    print(f\"load_factor - Objetivo: {target_values['load_factor']}, Simulado: {simulated_df['load_factor'].mean():.2f}\")\n",
    "\n",
    "    return simulated_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0efe9-fc0b-4ff6-876f-be0a8cc34ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mc_columns = op_vars + touchpoints\n",
    "mc_sim = simulate_client_satisfaction_with_beta(df_historic[mc_columns], targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6988a-8266-41ec-ad59-c690af683938",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Comprobación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d87763-ecb2-49c7-9842-0fc0299fd507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug_swaped_pipe = pd.read_csv('swaped_simulated_debug_pipe.csv')\n",
    "debug_swaped_pipe['cabin_in_surveyed_flight'] = 'Economy'\n",
    "debug_swaped_pipe['haul'] = 'SH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e09a2-175c-4ceb-bfa9-d1dc57ed9cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_probabilities_swaped = predict_and_explain(clf_model[model_names[0]], clf_model[model_names[1]], debug_swaped_pipe[columns], features, K_uncertainty=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c0679-55ad-447a-bdbf-c84e05d63c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_date_f = '2023-03-01'\n",
    "end_date_f = '2023-03-31'\n",
    "\n",
    "annual_df_sim = calculate_metrics_summary(df_probabilities_swaped, start_date_f, end_date_f, touchpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be70c545-e818-4cfa-92e7-11c734a8b10e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_probabilities_swaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b76b5-caed-46ab-a123-663f4930c231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annual_df_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43428e2-234b-4a2d-a9dc-37afc4acbb7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_probabilities_swaped = predict_and_explain(clf_model[model_names[0]], clf_model[model_names[1]], swaped_df[columns], features, K_uncertainty=5)\n",
    "df_probabilities_soft_sim = predict_and_explain(clf_model[model_names[0]], clf_model[model_names[1]], soft_sim_df[columns], features, K_uncertainty=5)\n",
    "# df_probabilities_hard_sim = predict_and_explain(clf_model[model_names[0]], clf_model[model_names[1]], hard_sim_df, features, K_uncertainty=5)\n",
    "\n",
    "# Añadir la columna 'simulation_client_type' a cada DataFrame\n",
    "df_probabilities_2023['simulation_client_type'] = 'original_2023'\n",
    "df_probabilities_2024['simulation_client_type'] = 'original_2024'\n",
    "df_probabilities_swaped['simulation_client_type'] = 'swaped_simulated'\n",
    "df_probabilities_soft_sim['simulation_client_type'] = 'soft_simulated'\n",
    "# df_probabilities_hard_sim['simulation_client_type'] = 'hard_simulated'\n",
    "\n",
    "# Concatenar los DataFrames\n",
    "client_df = pd.concat(\n",
    "    [df_probabilities_2023, df_probabilities_2024, df_probabilities_swaped, df_probabilities_soft_sim,],\n",
    "    ignore_index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76674198-d63a-401b-8f0a-7cab66b57366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_variable_against_list(df_probabilities_2024, main_var, [var for var in touchpoints if var != main_var], cluster_col='cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b56ed02-db0c-45e1-9b66-935656c80288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir las fechas de inicio y fin del mes\n",
    "start_date_o = '2023-06-01'\n",
    "end_date_o = '2023-08-30'\n",
    "\n",
    "start_date_f = '2024-06-01'\n",
    "end_date_f = '2024-08-30'\n",
    "\n",
    "# Filtrar el DataFrame por tipo de cliente y fechas del mes actual\n",
    "filtered_client_df_or = client_df[client_df['simulation_client_type'] == 'original_2023']\n",
    "filtered_client_df_sim = client_df[client_df['simulation_client_type'] == 'original_2024']\n",
    "\n",
    "# Calcular las métricas de resumen para el mes\n",
    "annual_df_or = calculate_metrics_summary(filtered_client_df_or, start_date_o, end_date_o, touchpoints)\n",
    "annual_df_sim = calculate_metrics_summary(filtered_client_df_sim, start_date_f, end_date_f, touchpoints)\n",
    "\n",
    "numerical_cols = annual_df_or.select_dtypes(include='number').columns\n",
    "\n",
    "diff_df = annual_df_sim[numerical_cols] - annual_df_or[numerical_cols]\n",
    "\n",
    "diff_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9822237d-2424-4a36-963a-4d1821037681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0204b-2649-4888-bba0-14189afc179e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copiar el DataFrame\n",
    "df = diff_df.copy()\n",
    "\n",
    "# Filtrar columnas que terminan en '_nps' excepto las columnas específicas\n",
    "nps_columns = [col for col in df.columns if col.endswith('_nps') and col not in ['uncertainty_nps', 'base_prob_nps', 'out_prob_nps']]\n",
    "\n",
    "# Calcular el valor de 'out_prob_nps'\n",
    "out_prob_nps = df['out_prob_nps'].values[0]\n",
    "\n",
    "# Crear una lista de contribuciones de cada columna a 'out_prob_nps'\n",
    "waterfall_data = df[nps_columns].iloc[0].tolist()\n",
    "\n",
    "# Obtener las etiquetas (nombres de las columnas) para el gráfico\n",
    "labels = nps_columns\n",
    "\n",
    "\n",
    "# Crear los valores del gráfico waterfall\n",
    "waterfall_values = waterfall_data\n",
    "\n",
    "# Crear las columnas correspondientes sin el sufijo \"_nps\" para los valores adicionales\n",
    "corresponding_columns = [col.replace('_nps', '') for col in nps_columns]\n",
    "\n",
    "# Obtener los valores correspondientes de las columnas sin el sufijo \"_nps\"\n",
    "corresponding_values = df[corresponding_columns].iloc[0].tolist()\n",
    "\n",
    "# Crear el gráfico waterfall con los valores adicionales encima de las barras\n",
    "def waterfall_plot(labels, values, corresponding_values, title=\"Original 2023 to Original 2024\"):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Crear barras de contribución\n",
    "    prev_value = 0\n",
    "    for i in range(len(values)):\n",
    "        color = 'green' if values[i] > 0 else 'red'\n",
    "        ax.bar(labels[i], values[i], bottom=prev_value, color=color)\n",
    "        \n",
    "        # Posición en la que agregar el texto\n",
    "        bar_position = prev_value + values[i] / 2\n",
    "        ax.text(i, bar_position, f'{corresponding_values[i]:.2f}', ha='center', va='center', color='black', fontsize=10)\n",
    "        \n",
    "        prev_value += values[i]\n",
    "\n",
    "    # Añadir etiquetas y título\n",
    "    ax.set_ylabel('Contributions to out_prob_nps')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Rotar etiquetas del eje X\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Llamar a la función para crear el gráfico con los valores adicionales\n",
    "waterfall_plot(labels, waterfall_values, corresponding_values)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.r5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
